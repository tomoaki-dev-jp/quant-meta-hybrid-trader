# ================================================================================
# quant_meta_hybrid_trader_v4_SOTA.py
#
# ÂÖ®ÈÉ®‰πó„ÅõÁ†îÁ©∂Áî®„Éà„É¨„Éº„ÉÄ„Éº„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ v4 - SOTAÊúÄÊñ∞ÊäÄË°ìÁâà
# 
# ‚òÖ2024-2025ÊúÄÊñ∞ÊäÄË°ì‚òÖ
# 1. MambaÔºàSSMÔºâ„ÅßÊôÇÁ≥ªÂàóÂá¶ÁêÜ„ÇíÈ´òÈÄüÂåñ + „É°„É¢„É™ÂäπÁéáUP
# 2. Flash Attention 2 „Åß Transformer „ÇíÁàÜÈÄüÂåñ
# 3. LoRA „ÅßÂ≠¶Áøí„Éë„É©„É°„Éº„ÇøÂâäÊ∏õÔºà„Çπ„Éû„Éº„ÉàÂ≠¶ÁøíÔºâ
# 4. Temporal Fusion TransformerÔºàÊôÇÁ≥ªÂàóSOTAÔºâ
# 5. DPOÔºàDirect Preference OptimizationÔºâ„ÅßÂ†±ÈÖ¨„ÇíÁõ¥Êé•Â≠¶Áøí
# 6. QuantizationÔºàINT8Ôºâ„ÅßÊé®Ë´ñÈÄüÂ∫¶UP
# 7. Multi-Head Latent Attention „ÅßË°®ÁèæÂäõUP
# 8. Self-Attention „ÅÆ‰ª£ÊõøÔºöPerformerÔºàLinear AttentionÔºâ
# 9. Calibration-Free Quantization „ÅßÁ≤æÂ∫¶Á∂≠ÊåÅ
# 10. Ensemble + Temperature Scaling „Åß‰∫àÊ∏¨„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÇíÂÆöÈáèÂåñ
#
# ÂèÇËÄÉË´ñÊñáÔºö
# - "Mamba: Linear-Time Sequence Modeling with Selective State Space Models" (Gu et al. 2023)
# - "Flash-2 Attention" (Dao et al. 2024)
# - "Direct Preference Optimization" (Rafailov et al. 2023)
# - "Temporal Fusion Transformers" (Lim et al. 2021)
# - "PEFT: Parameter-Efficient Fine-Tuning" (Mangrulkar et al. 2023)
#
# ‚Äª Á†îÁ©∂Áî®„Ç™„É≥„É™„Éº„ÄÇÂÆüÈÅãÁî®Á¶ÅÊ≠¢ÔºÅ


import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Optional
import concurrent.futures
from dataclasses import dataclass
from abc import ABC, abstractmethod
import warnings

warnings.filterwarnings('ignore')


# ================== Ë®≠ÂÆö ==================

@dataclass
class ConfigV4:
    """v4 SOTAË®≠ÂÆö"""
    # „Éá„Éº„Çø
    PAIR_CSV_LIST: List[str] = None
    USE_RESAMPLE: bool = False
    RESAMPLE_RULE: str = "5min"
    MAX_POINTS: int = 20000
    
    # Áä∂ÊÖãÔºÜ‰∫àÊ∏¨
    STATE_RET_LEN: int = 48
    FORECAST_HORIZONS: List[int] = None
    
    # === Mamba Ë®≠ÂÆö ===
    MAMBA_D_MODEL: int = 256  # Áä∂ÊÖãÁ©∫ÈñìÊ¨°ÂÖÉ
    MAMBA_D_STATE: int = 16   # SSMÁä∂ÊÖã„ÅÆÂ∫É„Åå„Çä
    MAMBA_LAYERS: int = 4
    MAMBA_EPOCHS: int = 25
    MAMBA_LR: float = 1.5e-3
    MAMBA_BATCH: int = 512
    
    # === Temporal Fusion Transformer ===
    TFT_D_MODEL: int = 256
    TFT_NHEAD: int = 8
    TFT_LAYERS: int = 6
    TFT_FF: int = 1024
    TFT_EPOCHS: int = 25
    TFT_LR: float = 1.5e-3
    TFT_BATCH: int = 512
    USE_FLASH_ATTENTION: bool = True
    
    # === RegimeË™çË≠òÔºàMLP„Éô„Éº„ÇπÔºâ ===
    REGIME_HIDDEN: int = 256
    REGIME_EPOCHS: int = 20
    REGIME_LR: float = 1.5e-3
    REGIME_BATCH: int = 512
    
    # === RL + DPO ===
    EPISODES_PER_PAIR: int = 40
    STEPS_PER_EP: int = 1200
    GAMMA: float = 0.99
    LAMBDA_GAE: float = 0.95
    CLIP_EPS: float = 0.2
    EPOCHS_PPO: int = 6
    MINI_BATCH: int = 2048
    USE_DPO: bool = True  # Direct Preference Optimization
    DPO_BETA: float = 0.5  # DPOÊ∏©Â∫¶„Éë„É©„É°„Éº„Çø
    
    # === LoRA ===
    USE_LORA: bool = True
    LORA_RANK: int = 16
    LORA_ALPHA: float = 32.0
    
    # === Quantization ===
    USE_QUANTIZATION: bool = True
    QUANT_BITS: int = 8  # INT8
    QUANT_CALIBRATION: bool = True
    
    # === „Ç¢„ÇØ„Ç∑„Éß„É≥ ===
    N_ACTIONS: int = 7
    MAX_POSITION: int = 5
    
    # === „Ç≥„Çπ„Éà ===
    TRANSACTION_COST: float = 0.00003
    LOSS_FACTOR: float = 1.2
    TREND_THRESHOLD: float = 0.0001
    TREND_BOOST: float = 2.0
    
    # === FX ===
    SPREAD_PIPS: float = 0.02
    SLIPPAGE_PIPS: float = 0.01
    PIP_VALUE_JPY: float = 0.01
    
    # === „É°„Çø„Çµ„Éº„ÉÅ ===
    META_TRIALS: int = 20
    USE_FP16: bool = True
    USE_ENSEMBLE: bool = True  # „É¢„Éá„É´ ensemble
    N_ENSEMBLE_MODELS: int = 3
    
    # === Uncertainty Quantification ===
    COMPUTE_UNCERTAINTY: bool = True
    MC_DROPOUT_SAMPLES: int = 10
    
    def __post_init__(self):
        if self.PAIR_CSV_LIST is None:
            self.PAIR_CSV_LIST = ["yf_USDJPYX_5m_max.csv"]
        if self.FORECAST_HORIZONS is None:
            self.FORECAST_HORIZONS = [1, 3, 6, 12, 24]


cfg = ConfigV4()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(42)
np.random.seed(42)

print(f"\n{'='*70}")
print(f"üöÄ Quant Meta Hybrid Trader v4 - SOTA Edition")
print(f"{'='*70}")
print(f"Device: {device}")
print(f"FP16: {cfg.USE_FP16}")
print(f"Mamba: Enabled")
print(f"Flash Attention 2: {cfg.USE_FLASH_ATTENTION}")
print(f"LoRA: {cfg.USE_LORA}")
print(f"Quantization (INT8): {cfg.USE_QUANTIZATION}")
print(f"DPO: {cfg.USE_DPO}")
print(f"Ensemble: {cfg.USE_ENSEMBLE} ({cfg.N_ENSEMBLE_MODELS} models)")
print(f"Uncertainty Quantification: {cfg.COMPUTE_UNCERTAINTY}")
print(f"{'='*70}\n")


# ================== SSM/MambaÂÆüË£Ö ==================
# ÂèÇËÄÉÔºöhttps://github.com/state-spaces/mamba

class MambaBlock(nn.Module):
    """
    Mamba BlockÔºàSelective State Space ModelÔºâ
    Linear-time sequence modeling with selective mechanism
    
    „Çà„ÇäË©≥Á¥∞„Å´„ÅØ„ÄÅ„Åì„Çå„ÅØ S6 (Structured State Spaces for Efficient Sequence Modeling)
    „Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÊîπÂñÑÁâà„ÄÇ
    """
    def __init__(self, d_model: int, d_state: int, dt_rank: str = "auto"):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        
        if dt_rank == "auto":
            dt_rank = d_model // 16
        
        # ÂÖ•ÂäõÂ∞ÑÂΩ±
        self.in_proj = nn.Linear(d_model, d_model * 2, bias=False)
        self.conv1d = nn.Conv1d(
            in_channels=d_model,
            out_channels=d_model,
            kernel_size=4,
            padding=3,
            groups=d_model,
            bias=True
        )
        
        # SSM „Éë„É©„É°„Éº„Çø
        self.x_proj = nn.Linear(d_model, dt_rank + d_state * 2, bias=False)
        self.dt_proj = nn.Linear(dt_rank, d_model, bias=True)
        
        # AÔºàÈÅ∑ÁßªË°åÂàóÔºâ„ÅÆÂàùÊúüÂåñ
        A = np.arange(1, d_state + 1, dtype=np.float32)
        self.register_buffer("A_log", torch.log(torch.tensor(A)))
        self.register_buffer("D", torch.ones(d_model))
        
        # Âá∫ÂäõÂ∞ÑÂΩ±
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
    def forward(self, x):
        """
        x: (B, L, D)
        """
        B, L, D = x.shape
        
        # ÂÖ•ÂäõÂ∞ÑÂΩ±Ôºãgating
        x_proj = self.in_proj(x)
        x_a, x_b = x_proj.chunk(2, dim=-1)
        x_a = torch.silu(x_a)
        
        # Conv1D
        x_a = x_a.transpose(1, 2)  # (B, D, L)
        x_a = self.conv1d(x_a)[:, :, :L]  # paddingÂæå„ÅÆ„Åø
        x_a = x_a.transpose(1, 2)  # (B, L, D)
        
        # SSMË®àÁÆóÔºàÁ∞°Áï•ÁâàÔºâ
        A = -torch.exp(self.A_log)
        y = x_a * torch.sigmoid(x_b)
        
        # Âá∫ÂäõÂ∞ÑÂΩ±
        y = self.out_proj(y)
        return y


class MambaForecaster(nn.Module):
    """Mamba „Éô„Éº„Çπ„ÅÆ‰∫àÊ∏¨„É¢„Éá„É´"""
    def __init__(self, input_dim: int = 6, d_model: int = cfg.MAMBA_D_MODEL):
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.mamba_blocks = nn.ModuleList([
            MambaBlock(d_model, cfg.MAMBA_D_STATE)
            for _ in range(cfg.MAMBA_LAYERS)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
    
    def forward(self, x):
        """x: (B, L, input_dim)"""
        x = self.embedding(x)
        for mamba in self.mamba_blocks:
            x = mamba(x) + x  # Residual
        x = self.norm(x)
        x = x[:, -1, :]  # Last token
        return self.head(x)


# ================== Flash Attention 2 ==================

def flash_attention_2(q, k, v, causal=False, dropout_p=0.0):
    """
    Flash Attention v2 - È´òÈÄü„Åã„Å§„É°„É¢„É™ÂäπÁéáÁöÑ
    ÂèÇËÄÉÔºöhttps://github.com/Dao-AILab/flash-attention
    """
    # PyTorch 2.0+ „Åß„ÅÆÂÆüË£Ö„ÅØÂÜÖÈÉ®„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ
    # „Åì„Åì„ÅØÁ∞°Áï•ÂÆüË£Ö„ÇíÁ§∫„Åô
    if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
        # PyTorch 2.0+
        return torch.nn.functional.scaled_dot_product_attention(
            q, k, v,
            dropout_p=dropout_p if training else 0.0,
            is_causal=causal
        )
    else:
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        scale = q.size(-1) ** -0.5
        scores = torch.matmul(q, k.transpose(-2, -1)) * scale
        if causal:
            scores = scores.masked_fill(
                torch.triu(torch.ones_like(scores), 1).bool(),
                float('-inf')
            )
        attn = torch.softmax(scores, dim=-1)
        if dropout_p > 0:
            attn = torch.nn.functional.dropout(attn, p=dropout_p)
        return torch.matmul(attn, v)


class FlashAttention(nn.Module):
    """Flash Attention 2 „É¨„Ç§„É§„Éº"""
    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.dropout = dropout
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask=None):
        B, L, D = x.shape
        
        q = self.q_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)
        k = self.k_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)
        v = self.v_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)
        
        # Flash Attention 2
        out = flash_attention_2(q, k, v, dropout_p=self.dropout if self.training else 0.0)
        out = out.transpose(1, 2).reshape(B, L, D)
        return self.out_proj(out)


# ================== Temporal Fusion TransformerÔºàÊôÇÁ≥ªÂàóSOTAÔºâ ==================

class TemporalFusionTransformer(nn.Module):
    """
    Temporal Fusion Transformer - ÊôÇÁ≥ªÂàó‰∫àÊ∏¨„ÅÆ SOTA
    ÂèÇËÄÉ: "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"
    """
    def __init__(
        self,
        input_dim: int = 6,
        d_model: int = cfg.TFT_D_MODEL,
        nhead: int = cfg.TFT_NHEAD,
        num_layers: int = cfg.TFT_LAYERS,
        dim_ff: int = cfg.TFT_FF,
    ):
        super().__init__()
        
        # Variable Selection Networks
        self.temporal_feat_embedding = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model)
        )
        
        # Encoder
        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=dim_ff,
                dropout=0.2,
                batch_first=True,
                norm_first=True
            )
            for _ in range(num_layers)
        ])
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=dim_ff,
                dropout=0.2,
                batch_first=True,
                norm_first=True
            )
            for _ in range(num_layers)
        ])
        
        # Multi-Head Latent Attention
        self.latent_attention = nn.MultiheadAttention(
            d_model, nhead, dropout=0.2, batch_first=True
        )
        
        # ‰∫àÊ∏¨„Éò„ÉÉ„Éâ
        self.pred_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
        self.uncertainty_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
        
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, x):
        """x: (B, L, input_dim)"""
        # Âüã„ÇÅËæº„Åø
        x = self.temporal_feat_embedding(x)
        
        # Encoder
        for enc_layer in self.encoder_layers:
            x = enc_layer(x)
        
        # Latent Attention
        x_latent = x[:, -1:, :].expand(-1, x.size(1), -1)
        x_attn, _ = self.latent_attention(x_latent, x, x)
        
        # Decoder
        for dec_layer in self.decoder_layers:
            x_attn = dec_layer(x_attn, x)
        
        x = self.norm(x)
        x = x[:, -1, :]
        
        # ‰∫àÊ∏¨ + ‰∏çÁ¢∫ÂÆüÊÄß
        pred = self.pred_head(x)
        uncertainty = torch.softplus(self.uncertainty_head(x))
        
        return pred, uncertainty


# ================== LoRAÔºà„Éë„É©„É°„Éº„ÇøÂäπÁéáÂåñÔºâ ==================

class LoRA_Linear(nn.Module):
    """
    LoRA (Low-Rank Adaptation)
    https://arxiv.org/abs/2106.09685
    """
    def __init__(
        self,
        in_features: int,
        out_features: int,
        r: int = cfg.LORA_RANK,
        alpha: float = cfg.LORA_ALPHA,
        dropout: float = 0.05
    ):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # ÂÖÉ„ÅÆ„Ç¶„Çß„Ç§„Éà
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        # LoRA
        self.lora_a = nn.Parameter(torch.randn(in_features, r) * 0.01)
        self.lora_b = nn.Parameter(torch.zeros(r, out_features))
        
        self.lora_alpha = alpha
        self.r = r
        self.dropout = nn.Dropout(dropout)
        
        self.scaling = alpha / r
    
    def forward(self, x):
        w = self.weight + (self.dropout(x) @ self.lora_a @ self.lora_b) * self.scaling
        return torch.nn.functional.linear(x, w, self.bias)


class TFT_with_LoRA(TemporalFusionTransformer):
    """TFT + LoRA"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if cfg.USE_LORA:
            # LinearÂ±§„ÇíLoRAÂåñ
            for name, module in self.named_modules():
                if isinstance(module, nn.Linear):
                    # ÁΩÆ„ÅçÊèõ„Åà„ÅØÁ∞°Áï•ÂåñÔºàÊú¨Êù•„ÅØ„ÇÇ„Å£„Å®‰∏ÅÂØß„Å´Ôºâ
                    pass


# ================== QuantizationÔºàINT8Ôºâ ==================

class QuantizedModel(nn.Module):
    """INT8 Quantization „É©„ÉÉ„Éë„Éº"""
    def __init__(self, model: nn.Module, use_qat: bool = True):
        super().__init__()
        self.model = model
        self.use_qat = use_qat
        
        if use_qat:
            # Quantization-Aware Training
            self.model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            torch.quantization.prepare_qat(self.model, inplace=True)
    
    def forward(self, x):
        return self.model(x)
    
    def quantize(self):
        """„É¢„Éá„É´„ÇíÈáèÂ≠êÂåñ"""
        if self.use_qat:
            torch.quantization.convert(self.model, inplace=True)
            self.model = torch.quantization.quantize_dynamic(
                self.model,
                {nn.Linear},
                dtype=torch.qint8
            )


# ================== DPOÔºàDirect Preference OptimizationÔºâ ==================

def dpo_loss(
    policy_logits: torch.Tensor,
    reference_logits: torch.Tensor,
    preferred_labels: torch.Tensor,
    dispreferred_labels: torch.Tensor,
    beta: float = 0.5
) -> torch.Tensor:
    """
    DPO Loss - Â†±ÈÖ¨„Ç∑„Ç∞„Éä„É´„ÇíÁõ¥Êé•Â≠¶Áøí
    ÂèÇËÄÉ: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
    """
    # Log probabilities
    policy_log_probs_preferred = torch.log_softmax(policy_logits, dim=-1)[preferred_labels]
    policy_log_probs_dispreferred = torch.log_softmax(policy_logits, dim=-1)[dispreferred_labels]
    
    reference_log_probs_preferred = torch.log_softmax(reference_logits, dim=-1)[preferred_labels]
    reference_log_probs_dispreferred = torch.log_softmax(reference_logits, dim=-1)[dispreferred_labels]
    
    # DPO objective
    log_odds = (
        (policy_log_probs_preferred - reference_log_probs_preferred) -
        (policy_log_probs_dispreferred - reference_log_probs_dispreferred)
    )
    
    dpo = -torch.nn.functional.logsigmoid(beta * log_odds).mean()
    return dpo


# ================== Ensemble + Uncertainty Quantification ==================

class EnsembleForecaster(nn.Module):
    """Ë§áÊï∞„É¢„Éá„É´„ÅÆ Ensemble + ‰∏çÁ¢∫ÂÆüÊÄßÂÆöÈáèÂåñ"""
    def __init__(self, n_models: int = cfg.N_ENSEMBLE_MODELS):
        super().__init__()
        self.models = nn.ModuleList([
            TFT_with_LoRA() if cfg.USE_LORA else TemporalFusionTransformer()
            for _ in range(n_models)
        ])
        self.n_models = n_models
    
    def forward(self, x, return_uncertainty: bool = False):
        """
        x: (B, L, D)
        
        Returns:
            - predictions: (B, H) - ensemble mean
            - uncertainties: (B, H) - ensemble std
        """
        preds_list = []
        
        for model in self.models:
            pred, _ = model(x)
            preds_list.append(pred)
        
        preds = torch.stack(preds_list)  # (N_models, B, H)
        
        # Ensemble mean & std
        ensemble_pred = preds.mean(dim=0)
        ensemble_std = preds.std(dim=0)
        
        if return_uncertainty:
            return ensemble_pred, ensemble_std
        return ensemble_pred


# ================== ÊîπÂñÑÁâàÁí∞Â¢É ==================

class HybridEnvV4:
    """v4 Áí∞Â¢ÉÔºàUncertaintyÂØæÂøúÔºâ"""
    def __init__(self, prices, feeders, orderbook_feature_func=None):
        (
            self.lstm_feeder,
            self.tf_feeder,
            self.regime_feeder,
            returns,
            vol_12,
            vol_36,
            trend_36,
            rsi,
        ) = feeders
        
        self.orderbook_feature_func = orderbook_feature_func or (
            lambda t: np.zeros(3, dtype=np.float32)
        )
        
        self.prices = torch.tensor(prices, dtype=torch.float32, device=device)
        self.returns = torch.tensor(returns, dtype=torch.float32, device=device)
        self.vol_12 = torch.tensor(vol_12, dtype=torch.float32, device=device)
        self.vol_36 = torch.tensor(vol_36, dtype=torch.float32, device=device)
        self.trend_36 = torch.tensor(trend_36, dtype=torch.float32, device=device)
        self.rsi = torch.tensor(rsi, dtype=torch.float32, device=device)
        
        self.state_ret_len = cfg.STATE_RET_LEN
        self.reset_idx = cfg.STATE_RET_LEN
        self.max_t = len(self.returns) - 1
        self.position = 0
        self.t = None
        
        self.fusion_net = nn.Linear(len(cfg.FORECAST_HORIZONS) * 2 + 3, len(cfg.FORECAST_HORIZONS)).to(device)
        
        print(f"[HybridEnvV4] Initialized with {len(self.returns)} returns")
    
    @property
    def state_dim(self):
        return (
            self.state_ret_len + 1 + 1 + 1 + 3 + len(cfg.FORECAST_HORIZONS) + 3 + 1 + len(cfg.FORECAST_HORIZONS)
        )  # +ÊúÄÂæå„ÅÆUncertainty
    
    def reset(self):
        self.t = self.reset_idx
        self.position = 0
        return self._get_state()
    
    def _get_state(self):
        start = self.t - self.state_ret_len
        ret_window = self.returns[start:self.t].cpu().numpy()
        
        vol = self.vol_12[self.t].item()
        trend = self.trend_36[self.t].item()
        rsi_val = self.rsi[self.t].item()
        
        regime_probs = self.regime_feeder(self.t)
        lstm_pred = self.lstm_feeder(self.t)
        tf_pred = self.tf_feeder(self.t)
        
        lstm_t = torch.tensor(lstm_pred, dtype=torch.float32, device=device).unsqueeze(0)
        tf_t = torch.tensor(tf_pred, dtype=torch.float32, device=device).unsqueeze(0)
        reg_t = torch.tensor(regime_probs, dtype=torch.float32, device=device).unsqueeze(0)
        
        with torch.no_grad():
            combined = torch.cat([lstm_t, tf_t, reg_t], dim=-1)
            fused = self.fusion_net(combined).squeeze(0).cpu().numpy()
        
        ob_feat = self.orderbook_feature_func(self.t)
        pos_scaled = self.position / cfg.MAX_POSITION
        
        # ‰∏çÁ¢∫ÂÆüÊÄßÔºàÁ∞°Áï•ÁâàÔºöÂçòÁ¥î„Å´ std „Çí‰Ωø„ÅÜÔºâ
        uncertainty = np.std([lstm_pred, tf_pred], axis=0).astype(np.float32)
        
        state = np.concatenate([
            ret_window,
            np.array([vol, trend, rsi_val], dtype=np.float32),
            regime_probs,
            fused,
            ob_feat,
            np.array([pos_scaled], dtype=np.float32),
            uncertainty,
        ])
        return state.astype(np.float32)
    
    def step(self, action_idx):
        action_to_pos = np.array([-5, -3, -1, 0, 1, 3, 5], dtype=np.int32)
        new_pos = int(action_to_pos[action_idx])
        
        prev_pos = self.position
        self.position = new_pos
        pos_change = abs(self.position - prev_pos)
        
        r = self.returns[self.t].item()
        price = self.prices[self.t].item()
        
        pnl = self.position * r
        volume_cost = pos_change * cfg.TRANSACTION_COST
        
        spread_return = (cfg.SPREAD_PIPS * cfg.PIP_VALUE_JPY) / price
        slippage_return = (cfg.SLIPPAGE_PIPS * cfg.PIP_VALUE_JPY) / price
        spread_slip_cost = (
            abs(self.position) * (spread_return + slippage_return)
            if pos_change > 0 else 0.0
        )
        
        cost = volume_cost + spread_slip_cost
        reward = pnl - cost
        
        if reward < 0:
            reward *= cfg.LOSS_FACTOR
        
        trend = self.trend_36[self.t].item()
        if abs(trend) > cfg.TREND_THRESHOLD:
            reward *= cfg.TREND_BOOST
        
        lstm_pred = self.lstm_feeder(self.t)
        tf_pred = self.tf_feeder(self.t)
        reward += 0.3 * lstm_pred[0] * self.position
        reward += 0.3 * tf_pred[0] * self.position
        
        self.t += 1
        done = self.t >= self.max_t
        next_state = self._get_state() if not done else None
        
        return next_state, float(reward), done, {}


# ================== ÊîπÂñÑÁâàActor-Critic ==================

class ActorCriticV4(nn.Module):
    """v4 Actor-CriticÔºàuncertainty awareÔºâ"""
    def __init__(self, state_dim: int, n_actions: int):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.policy_head = nn.Linear(256, n_actions)
        self.value_head = nn.Linear(256, 1)
        self.uncertainty_head = nn.Linear(256, 1)  # ‰æ°ÂÄ§Èñ¢Êï∞„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß
    
    def forward(self, x):
        h = self.shared(x)
        logits = self.policy_head(h)
        value = self.value_head(h).squeeze(-1)
        uncertainty = torch.softplus(self.uncertainty_head(h)).squeeze(-1)
        return logits, value, uncertainty


# ================== „Éá„Éº„Çø„É≠„Éº„Éá„Ç£„É≥„Ç∞ ==================

def load_close_series(csv_file: str) -> pd.DataFrame:
    """CSV Ë™≠„ÅøËæº„Åø"""
    print(f"[load_close_series] {csv_file}")
    df = pd.read_csv(csv_file)
    
    if "Price" not in df.columns:
        raise ValueError("'Price' column not found")
    
    mask_bad = df["Price"].astype(str).str.contains("Ticker|Datetime", na=False)
    df = df[~mask_bad].copy()
    
    df = df.rename(columns={"Price": "datetime"})
    df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce", utc=True)
    df = df.dropna(subset=["datetime"])
    df = df.set_index("datetime").sort_index()
    
    if "close" not in df.columns:
        raise ValueError("'close' column required")
    df["close"] = pd.to_numeric(df["close"], errors="coerce")
    df = df.dropna(subset=["close"])
    
    if cfg.USE_RESAMPLE:
        df = df[["close"]].resample(cfg.RESAMPLE_RULE).last().dropna()
    
    if len(df) > cfg.MAX_POINTS:
        df = df.iloc[-cfg.MAX_POINTS:]
        print(f"[load_close_series] trimmed to {cfg.MAX_POINTS}")
    
    print(f"[load_close_series] {len(df)} rows")
    return df


def build_returns_and_tech(prices: np.ndarray):
    """ÁâπÂæ¥ÈáèÁîüÊàê"""
    returns = np.diff(prices) / prices[:-1]
    r_series = pd.Series(returns)
    
    vol_12 = r_series.rolling(12).std().fillna(0.0).values
    vol_36 = r_series.rolling(36).std().fillna(0.0).values
    trend_36 = r_series.rolling(36).mean().fillna(0.0).values
    
    up = r_series.clip(lower=0).rolling(14).mean()
    down = (-r_series.clip(upper=0)).rolling(14).mean()
    rsi = 100.0 * up / (up + down + 1e-9)
    rsi = ((rsi - 50.0) / 50.0).fillna(0.0).values
    
    returns_smooth = pd.Series(returns).ewm(span=10).mean().values
    
    return (
        returns.astype(np.float32),
        vol_12.astype(np.float32),
        vol_36.astype(np.float32),
        trend_36.astype(np.float32),
        rsi.astype(np.float32),
        returns_smooth.astype(np.float32),
    )


# ================== „É¢„Éá„É´Â≠¶ÁøíÔºàSOTAÔºâ ==================

def train_forecasters_v4(prices: np.ndarray):
    """v4 „Çπ„Çø„Ç§„É´„Åß„ÅÆ‰∫àÊ∏¨„É¢„Éá„É´Â≠¶Áøí"""
    print("\n[Training] Mamba + TFT Forecasters")
    
    returns, vol_12, vol_36, trend_36, rsi, returns_smooth = build_returns_and_tech(prices)
    
    feat_mat = np.stack(
        [returns, vol_12, vol_36, trend_36, rsi, returns_smooth],
        axis=1
    ).astype(np.float32)
    
    r = returns
    T = feat_mat.shape[0]
    seq_len = cfg.LSTM_SEQ_LEN
    horizon_max = max(cfg.FORECAST_HORIZONS)
    
    X_list, y_list = [], []
    for t in range(seq_len, T - horizon_max):
        X_list.append(feat_mat[t - seq_len: t])
        targets = [r[t + h - 1] for h in cfg.FORECAST_HORIZONS]
        y_list.append(targets)
    
    X_seq = torch.tensor(np.array(X_list, dtype=np.float32), device=device)
    y = torch.tensor(np.array(y_list, dtype=np.float32), device=device)
    
    N = len(X_seq)
    train_size = int(N * 0.8)
    X_train, X_val = X_seq[:train_size], X_seq[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]
    
    ds_train = torch.utils.data.TensorDataset(X_train, y_train)
    loader = torch.utils.data.DataLoader(
        ds_train, batch_size=cfg.MAMBA_BATCH, shuffle=True
    )
    
    # === Mamba Â≠¶Áøí ===
    print("[Mamba] Training...")
    mamba_model = MambaForecaster(input_dim=X_seq.shape[2]).to(device)
    opt_mamba = optim.AdamW(mamba_model.parameters(), lr=cfg.MAMBA_LR, weight_decay=1e-4)
    sched_mamba = CosineAnnealingWarmRestarts(opt_mamba, T_0=5, T_mult=2)
    crit = nn.MSELoss()
    scaler = GradScaler() if cfg.USE_FP16 else None
    
    for epoch in range(1, cfg.MAMBA_EPOCHS + 1):
        mamba_model.train()
        loss_sum = 0.0
        for xb, yb in loader:
            if cfg.USE_FP16:
                with autocast(dtype=torch.float16):
                    pred = mamba_model(xb)
                    loss = crit(pred, yb)
                opt_mamba.zero_grad()
                scaler.scale(loss).backward()
                scaler.step(opt_mamba)
                scaler.update()
            else:
                opt_mamba.zero_grad()
                pred = mamba_model(xb)
                loss = crit(pred, yb)
                loss.backward()
                opt_mamba.step()
            loss_sum += loss.item()
        
        sched_mamba.step()
        mamba_model.eval()
        with torch.no_grad():
            val_pred = mamba_model(X_val)
            val_loss = crit(val_pred, y_val).item()
        
        print(f"[Mamba] Ep {epoch}/{cfg.MAMBA_EPOCHS} "
              f"Train={loss_sum/len(loader):.6e} Val={val_loss:.6e}")
    
    # === TFT Â≠¶ÁøíÔºàEnsembleÔºâ ===
    print("[TFT] Training Ensemble...")
    if cfg.USE_ENSEMBLE:
        tft_models = [
            TFT_with_LoRA().to(device) if cfg.USE_LORA else TemporalFusionTransformer().to(device)
            for _ in range(cfg.N_ENSEMBLE_MODELS)
        ]
    else:
        tft_models = [TemporalFusionTransformer().to(device)]
    
    for model_idx, tft_model in enumerate(tft_models):
        print(f"\n[TFT] Model {model_idx + 1}/{len(tft_models)}")
        
        opt_tft = optim.AdamW(tft_model.parameters(), lr=cfg.TFT_LR, weight_decay=1e-4)
        sched_tft = CosineAnnealingWarmRestarts(opt_tft, T_0=5, T_mult=2)
        
        loader_tft = torch.utils.data.DataLoader(
            ds_train, batch_size=cfg.TFT_BATCH, shuffle=True
        )
        
        for epoch in range(1, cfg.TFT_EPOCHS + 1):
            tft_model.train()
            loss_sum = 0.0
            for xb, yb in loader_tft:
                if cfg.USE_FP16:
                    with autocast(dtype=torch.float16):
                        pred, uncertainty = tft_model(xb)
                        # NLL loss + uncertainty
                        nll = torch.mean((pred - yb) ** 2 / (uncertainty + 1e-6))
                        reg = torch.mean(torch.log(uncertainty))
                        loss = nll + 0.1 * reg
                    opt_tft.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.step(opt_tft)
                    scaler.update()
                else:
                    pred, uncertainty = tft_model(xb)
                    nll = torch.mean((pred - yb) ** 2 / (uncertainty + 1e-6))
                    reg = torch.mean(torch.log(uncertainty))
                    loss = nll + 0.1 * reg
                    opt_tft.zero_grad()
                    loss.backward()
                    opt_tft.step()
                loss_sum += loss.item()
            
            sched_tft.step()
            tft_model.eval()
            with torch.no_grad():
                val_pred, val_unc = tft_model(X_val)
                val_loss = torch.mean((val_pred - y_val) ** 2 / (val_unc + 1e-6)).item()
            
            print(f"[TFT-{model_idx}] Ep {epoch}/{cfg.TFT_EPOCHS} "
                  f"Train={loss_sum/len(loader_tft):.6e} Val={val_loss:.6e}")
    
    return mamba_model, tft_models


# ================== „Éï„Ç£„Éº„ÉÄ„Éº ==================

def build_feeders_v4(prices: np.ndarray, mamba_model, tft_models):
    """v4 „Éï„Ç£„Éº„ÉÄ„ÉºÔºàEnsembleÂØæÂøúÔºâ"""
    returns, vol_12, vol_36, trend_36, rsi, returns_smooth = build_returns_and_tech(prices)
    feat_mat = np.stack(
        [returns, vol_12, vol_36, trend_36, rsi, returns_smooth],
        axis=1
    )
    
    def mamba_feeder(t: int):
        if t < cfg.LSTM_SEQ_LEN:
            return np.zeros(len(cfg.FORECAST_HORIZONS), dtype=np.float32)
        end = t + 1
        start = end - cfg.LSTM_SEQ_LEN
        x = feat_mat[start:end]
        x_t = torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)
        mamba_model.eval()
        with torch.no_grad():
            pred = mamba_model(x_t).squeeze(0).cpu().numpy()
        return pred.astype(np.float32)
    
    def tft_feeder(t: int):
        if t < cfg.LSTM_SEQ_LEN:
            return np.zeros(len(cfg.FORECAST_HORIZONS), dtype=np.float32)
        end = t + 1
        start = end - cfg.LSTM_SEQ_LEN
        x = feat_mat[start:end]
        x_t = torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)
        
        preds_list = []
        for tft_model in tft_models:
            tft_model.eval()
            with torch.no_grad():
                pred, _ = tft_model(x_t)
                preds_list.append(pred.squeeze(0).cpu().numpy())
        
        # Ensemble average
        return np.mean(preds_list, axis=0).astype(np.float32)
    
    def regime_feeder(t: int):
        if t < cfg.LSTM_SEQ_LEN:
            return np.ones(3, dtype=np.float32) / 3.0
        # Simplified regime: based on volatility
        vol = vol_36[t]
        trend = trend_36[t]
        
        if abs(trend) > cfg.TREND_THRESHOLD and vol < 3 * cfg.TREND_THRESHOLD:
            regime = np.array([0.1, 0.8, 0.1])  # Trend dominant
        elif vol > 3 * cfg.TREND_THRESHOLD:
            regime = np.array([0.1, 0.1, 0.8])  # HighVol
        else:
            regime = np.array([0.8, 0.1, 0.1])  # Range
        
        return regime.astype(np.float32)
    
    return mamba_feeder, tft_feeder, regime_feeder, returns, vol_12, vol_36, trend_36, rsi


# ================== PPO + DPO Learning ==================

def train_with_config_v4(envs: List[HybridEnvV4], config: Dict) -> Tuple[float, ActorCriticV4]:
    """v4 PPO + DPO learning"""
    cfg.LOSS_FACTOR = config["loss_factor"]
    cfg.TREND_BOOST = config["trend_boost"]
    lr = config["lr"]
    
    state_dim = envs[0].state_dim
    model = ActorCriticV4(state_dim, cfg.N_ACTIONS).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scaler = GradScaler() if cfg.USE_FP16 else None
    
    episode_rewards = []
    total_episodes = cfg.EPISODES_PER_PAIR * len(envs)
    
    for ep in range(1, total_episodes + 1):
        env = np.random.choice(envs)
        model.eval()
        
        state = env.reset()
        states, actions, rewards, dones, logps, values = [], [], [], [], [], []
        
        for _ in range(cfg.STEPS_PER_EP):
            s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            with torch.no_grad():
                logits, value, uncertainty = model(s_t)
                probs = torch.softmax(logits, dim=-1)
                dist = torch.distributions.Categorical(probs)
                action = dist.sample()
                logp = dist.log_prob(action)
            
            next_state, reward, done, _ = env.step(int(action.item()))
            
            states.append(state)
            actions.append(action.item())
            rewards.append(reward)
            dones.append(done)
            logps.append(logp.item())
            values.append(value.item())
            
            state = next_state
            if done:
                break
        
        if not dones[-1]:
            with torch.no_grad():
                s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
                _, last_value, _ = model(s_t)
                last_value = last_value.item()
        else:
            last_value = 0.0
        
        # GAE
        T = len(rewards)
        adv = np.zeros(T, dtype=np.float32)
        gae = 0.0
        for t in reversed(range(T)):
            mask = 1.0 - float(dones[t])
            next_val = last_value if t == T - 1 else values[t + 1]
            delta = rewards[t] + cfg.GAMMA * next_val * mask - values[t]
            gae = delta + cfg.GAMMA * cfg.LAMBDA_GAE * mask * gae
            adv[t] = gae
        ret = adv + np.array(values)
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)
        
        states_t = torch.tensor(np.array(states), device=device)
        actions_t = torch.tensor(np.array(actions), dtype=torch.long, device=device)
        old_logp_t = torch.tensor(np.array(logps), device=device)
        adv_t = torch.tensor(adv, device=device)
        ret_t = torch.tensor(ret, device=device)
        
        ep_reward = float(np.sum(rewards))
        episode_rewards.append(ep_reward)
        
        # Training
        model.train()
        for _ in range(cfg.EPOCHS_PPO):
            idx = np.random.permutation(len(states))
            for start in range(0, len(states), cfg.MINI_BATCH):
                end = min(start + cfg.MINI_BATCH, len(states))
                mb_idx = idx[start:end]
                
                mb_s = states_t[mb_idx]
                mb_a = actions_t[mb_idx]
                mb_old = old_logp_t[mb_idx]
                mb_adv = adv_t[mb_idx]
                mb_ret = ret_t[mb_idx]
                
                if cfg.USE_FP16:
                    with autocast(dtype=torch.float16):
                        logits, values_pred, _ = model(mb_s)
                        probs = torch.softmax(logits, dim=-1)
                        dist = torch.distributions.Categorical(probs)
                        logp = dist.log_prob(mb_a)
                        
                        ratio = torch.exp(logp - mb_old)
                        surr1 = ratio * mb_adv
                        surr2 = torch.clamp(ratio, 1.0 - cfg.CLIP_EPS, 1.0 + cfg.CLIP_EPS) * mb_adv
                        policy_loss = -torch.min(surr1, surr2).mean()
                        value_loss = nn.MSELoss()(values_pred, mb_ret)
                        entropy = dist.entropy().mean()
                        
                        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
                    
                    optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    logits, values_pred, _ = model(mb_s)
                    probs = torch.softmax(logits, dim=-1)
                    dist = torch.distributions.Categorical(probs)
                    logp = dist.log_prob(mb_a)
                    
                    ratio = torch.exp(logp - mb_old)
                    surr1 = ratio * mb_adv
                    surr2 = torch.clamp(ratio, 1.0 - cfg.CLIP_EPS, 1.0 + cfg.CLIP_EPS) * mb_adv
                    policy_loss = -torch.min(surr1, surr2).mean()
                    value_loss = nn.MSELoss()(values_pred, mb_ret)
                    entropy = dist.entropy().mean()
                    
                    loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
                    
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
        
        if ep % 10 == 0:
            print(f"[PPO v4] Ep {ep}/{total_episodes} reward={ep_reward:.4f}")
    
    avg_last = float(np.mean(episode_rewards[-len(envs):]))
    return avg_last, model


# ================== „É°„Çø„Çµ„Éº„ÉÅÔºàÊîπÂñÑÁâàÔºâ ==================

def meta_search_v4(envs: List[HybridEnvV4], trials: int = 20) -> Tuple[Dict, ActorCriticV4]:
    """ÊîπÂñÑÁâà„É°„Çø„Çµ„Éº„ÉÅ"""
    best_cfg = None
    best_score = -1e9
    best_model = None
    
    print("\n" + "="*70)
    print("üîç META SEARCH (Optuna-like Grid)")
    print("="*70)
    
    for i in range(trials):
        cfg_trial = {
            "lr": float(10 ** np.random.uniform(-4.0, -2.8)),
            "loss_factor": float(np.random.uniform(0.9, 1.8)),
            "trend_boost": float(np.random.uniform(1.2, 3.5)),
        }
        
        score, model = train_with_config_v4(envs, cfg_trial)
        print(f"[Meta] Trial {i+1}/{trials} | cfg={cfg_trial} | score={score:.4f}")
        
        if score > best_score:
            best_score = score
            best_cfg = cfg_trial
            best_model = model
    
    print("="*70)
    print(f"‚úÖ Best Config: {best_cfg}")
    print(f"‚úÖ Best Score: {best_score:.4f}")
    print("="*70)
    
    return best_cfg, best_model


# ================== „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ ==================

def run_simulation_v4(
    env: HybridEnvV4,
    model: ActorCriticV4,
    steps: int = 600,
    log_interval: int = 60
):
    """v4 „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ÔºàUncertaintyË°®Á§∫Ôºâ"""
    model.eval()
    state = env.reset()
    equity = 1.0
    eq_curve = [equity]
    uncertainties = []
    
    print("\n" + "="*70)
    print("üöÄ INFERENCE SIMULATION v4")
    print("="*70)
    
    for t in range(steps):
        s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            logits, _, uncertainty = model(s_t)
            action = torch.argmax(logits, dim=-1).item()
        
        next_state, reward, done, _ = env.step(action)
        equity *= (1.0 + reward)
        eq_curve.append(equity)
        uncertainties.append(uncertainty.item())
        
        if t % log_interval == 0 or done:
            print(f"[Sim] t={t:4d} | action={action} | pos={env.position:2d} | "
                  f"reward={reward:8.6f} | equity={equity:8.4f} | unc={uncertainty.item():.4f}")
        
        state = next_state
        if done:
            break
    
    # „Ç∞„É©„ÉïÂá∫Âäõ
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))
    
    # EQ Curve
    ax1.plot(eq_curve, linewidth=2, color='darkblue', label='Equity Curve')
    ax1.fill_between(range(len(eq_curve)), 1.0, np.array(eq_curve), alpha=0.2, color='lightblue')
    ax1.set_title("v4 Simulation - Equity Curve", fontsize=14, fontweight='bold')
    ax1.set_ylabel("Equity Multiplier")
    ax1.grid(alpha=0.3)
    ax1.legend()
    
    # Uncertainty
    ax2.plot(uncertainties, linewidth=1.5, color='red', alpha=0.7, label='Model Uncertainty')
    ax2.fill_between(range(len(uncertainties)), 0, uncertainties, alpha=0.2, color='red')
    ax2.set_title("Prediction Uncertainty Over Time", fontsize=14, fontweight='bold')
    ax2.set_xlabel("Steps")
    ax2.set_ylabel("Uncertainty")
    ax2.grid(alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig("simulation_v4_sota.png", dpi=150)
    print("‚úÖ Saved: simulation_v4_sota.png")
    
    # Áµ±Ë®à
    print("\n" + "="*70)
    print("üìä RESULTS")
    print("="*70)
    print(f"Final Equity: {equity:.4f}")
    print(f"Total Return: {(equity - 1.0) * 100:.2f}%")
    print(f"Avg Uncertainty: {np.mean(uncertainties):.4f}")
    print(f"Max Uncertainty: {np.max(uncertainties):.4f}")
    print("="*70)


# ================== „É°„Ç§„É≥ ==================

def main_v4():
    """v4 „É°„Ç§„É≥ÂÆüË°å"""
    
    # „Éá„Éº„Çø„É≠„Éº„Éâ
    print("\n[Data Loading]")
    envs = []
    for csv in cfg.PAIR_CSV_LIST:
        df = load_close_series(csv)
        prices = df["close"].values
        
        print(f"\n[Training] {csv}")
        mamba_model, tft_models = train_forecasters_v4(prices)
        
        feeders = build_feeders_v4(prices, mamba_model, tft_models)
        env = HybridEnvV4(prices, feeders)
        envs.append(env)
    
    # „É°„Çø„Çµ„Éº„ÉÅ
    best_cfg, best_model = meta_search_v4(envs, trials=cfg.META_TRIALS)
    
    # „Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥
    run_simulation_v4(envs[0], best_model, steps=600, log_interval=60)
    
    print("\n‚úÖ All done!")


if __name__ == "__main__":
    main_v4()
