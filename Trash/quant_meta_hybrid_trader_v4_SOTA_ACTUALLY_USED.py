# ================================================================================
# quant_meta_hybrid_trader_v4_SOTA_ACTUALLY_USED.py
#
# v4 - SOTAÊúÄÊñ∞ÊäÄË°ìÁâàÔºà"Ê©üËÉΩ„ÅåÁîü„Åç„Çã" ‰øÆÊ≠£ÁâàÔºâ
#
# ‚úÖ ÂèçÊò†„Åó„Åü‰øÆÊ≠£
# - LoRA: TFTÂÜÖÈÉ®„ÅÆnn.Linear„ÇíLoRA_Linear„Å∏Ëá™ÂãïÁΩÆÊèõÔºàÊ≥®ÂÖ•Ôºâ
# - FlashAttention: latent attention„Å´FlashAttention„Çí‰ΩøÁî®ÔºàSDPA„ÇÇ‰ΩµÁî®Ôºâ
# - EnsembleForecaster: ÂÆüÈöõ„Å´Âà©Áî®„Åó„Å¶‰∫àÊ∏¨Ôºã‰∏çÁ¢∫ÂÆüÊÄß„ÇíËøî„Åô
# - Quantization(INT8): Êé®Ë´ñÁî®„Å´dynamic quantize„ÇíÈÅ©Áî®ÔºàCPUÊé®Ë´ñÂêë„ÅëÔºâ
# - DPO: PPOÂæå„Å´Á∞°ÊòìDPO„Çπ„ÉÜ„ÉÉ„ÉóÔºàPreferenceÂ≠¶ÁøíÔºâ„ÇíËøΩÂä†
# - ÈáçË§áÈñ¢Êï∞„ÅÆÂâäÈô§Ôºàload_close_series/build_returns_and_tech„ÅÆ‰∫åÈáçÂÆöÁæ©„ÇíËß£Ê∂àÔºâ
#
# ‚ö† Ê≥®ÊÑè
# - „ÄåPerformer„Äç„ÄåCalibration-Free Quant„Äç„ÄåTemperature Scaling„Äç„ÅØ
#   Êú¨Ê†ºÂÆüË£Ö„ÅåÈáç„ÅÑ„Åü„ÇÅ„ÄÅ„Åì„Åì„Åß„ÅØ"ÂÆüË£Ö„Éï„ÉÉ„ÇØ"„ÅÆ„ÅøÔºàREADME„Åß‚Äú„Éï„ÉÉ„ÇØ„ÅÇ„Çä‚Äù„Å®„Åó„Å¶Êâ±„ÅÜ„ÅÆ„ÅåÊ≠£Áõ¥Ôºâ
# - Quantization„ÅØGPU„ÅßÈ´òÈÄüÂåñ„Åô„Çã„Å®„ÅÑ„ÅÜ„Çà„Çä„ÄÅCPUÊé®Ë´ñ„ÅßÂäπ„Åè
# ================================================================================

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional
import warnings

warnings.filterwarnings("ignore")

# ================== Ë®≠ÂÆö ==================

@dataclass
class ConfigV4:
    # Data
    PAIR_CSV_LIST: List[str] = None
    USE_RESAMPLE: bool = False
    RESAMPLE_RULE: str = "5min"
    MAX_POINTS: int = 20000

    # State / Forecast
    STATE_RET_LEN: int = 48
    FORECAST_HORIZONS: List[int] = None

    # Mamba
    MAMBA_SEQ_LEN: int = 64
    MAMBA_D_MODEL: int = 256
    MAMBA_D_STATE: int = 16
    MAMBA_LAYERS: int = 4
    MAMBA_EPOCHS: int = 25
    MAMBA_LR: float = 1.5e-3
    MAMBA_BATCH: int = 512

    # TFT
    TFT_D_MODEL: int = 256
    TFT_NHEAD: int = 8
    TFT_LAYERS: int = 6
    TFT_FF: int = 1024
    TFT_EPOCHS: int = 25
    TFT_LR: float = 1.5e-3
    TFT_BATCH: int = 512
    USE_FLASH_ATTENTION: bool = True

    # Regime (simple prob)
    REGIME_EPS: float = 1e-6

    # RL + DPO
    EPISODES_PER_PAIR: int = 40
    STEPS_PER_EP: int = 1200
    GAMMA: float = 0.99
    LAMBDA_GAE: float = 0.95
    CLIP_EPS: float = 0.2
    EPOCHS_PPO: int = 6
    MINI_BATCH: int = 2048

    USE_DPO: bool = True
    DPO_BETA: float = 0.5
    DPO_STEPS_PER_EP: int = 2          # PPOÊõ¥Êñ∞„ÅÆÂæå„Å´DPO„Çí‰ΩïÂõûÂõû„Åô„ÅãÔºàËªΩÈáèÔºâ
    DPO_NUM_PAIRS: int = 512           # Preference„Éö„Ç¢Êï∞ÔºàËªΩÈáèÔºâ
    DPO_LR_MULT: float = 0.5           # PPO„ÅÆLR„Å´ÂØæ„Åô„ÇãDPOÁî®ÂÄçÁéá

    # LoRA
    USE_LORA: bool = True
    LORA_RANK: int = 16
    LORA_ALPHA: float = 32.0

    # Quantization
    USE_QUANTIZATION: bool = True
    QUANT_BITS: int = 8  # dynamic quantize only supports int8 practically

    # Actions
    N_ACTIONS: int = 7
    MAX_POSITION: int = 5

    # Costs
    TRANSACTION_COST: float = 0.00003
    LOSS_FACTOR: float = 1.2
    TREND_THRESHOLD: float = 0.0001
    TREND_BOOST: float = 2.0

    # FX
    SPREAD_PIPS: float = 0.02
    SLIPPAGE_PIPS: float = 0.01
    PIP_VALUE_JPY: float = 0.01

    # Meta search
    META_TRIALS: int = 20
    USE_FP16: bool = True

    # Ensemble / Uncertainty
    USE_ENSEMBLE: bool = True
    N_ENSEMBLE_MODELS: int = 3
    COMPUTE_UNCERTAINTY: bool = True

    def __post_init__(self):
        if self.PAIR_CSV_LIST is None:
            self.PAIR_CSV_LIST = ["yf_USDJPYX_5m_max.csv"]
        if self.FORECAST_HORIZONS is None:
            self.FORECAST_HORIZONS = [1, 3, 6, 12, 24]

cfg = ConfigV4()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.manual_seed(42)
np.random.seed(42)

print("\n" + "=" * 70)
print("üöÄ Quant Meta Hybrid Trader v4 - SOTA Edition (ACTUALLY USED)")
print("=" * 70)
print(f"Device: {device}")
print(f"FP16: {cfg.USE_FP16}")
print(f"Mamba: Enabled")
print(f"Flash Attention 2: {cfg.USE_FLASH_ATTENTION}")
print(f"LoRA: {cfg.USE_LORA}")
print(f"Quantization(INT8): {cfg.USE_QUANTIZATION}")
print(f"DPO: {cfg.USE_DPO}")
print(f"Ensemble: {cfg.USE_ENSEMBLE} ({cfg.N_ENSEMBLE_MODELS} models)")
print(f"Uncertainty: {cfg.COMPUTE_UNCERTAINTY}")
print("=" * 70 + "\n")


# ================== Data utils ==================

def load_close_series(csv_file: str) -> pd.DataFrame:
    df = pd.read_csv(csv_file)
    if "Price" not in df.columns:
        raise ValueError("CSV„Å´ 'Price' Âàó„ÅåË¶ãÂΩì„Åü„Çä„Åæ„Åõ„Çì„ÄÇ")

    mask_bad = df["Price"].astype(str).str.contains("Ticker|Datetime", na=False)
    df = df[~mask_bad].copy()

    df = df.rename(columns={"Price": "datetime"})
    df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce", utc=True)
    df = df.dropna(subset=["datetime"]).set_index("datetime").sort_index()

    if "close" not in df.columns:
        raise ValueError("CSV„Å´ close Âàó„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ")
    df["close"] = pd.to_numeric(df["close"], errors="coerce")
    df = df.dropna(subset=["close"])

    if cfg.USE_RESAMPLE:
        df = df[["close"]].resample(cfg.RESAMPLE_RULE).last().dropna()

    if len(df) > cfg.MAX_POINTS:
        df = df.iloc[-cfg.MAX_POINTS:].copy()

    print(f"[load_close_series] {csv_file} rows={len(df)}")
    return df


def build_returns_and_tech(prices: np.ndarray):
    returns = np.diff(prices) / prices[:-1]
    r = pd.Series(returns)

    vol_12 = r.rolling(12).std().fillna(0.0).values
    vol_36 = r.rolling(36).std().fillna(0.0).values
    trend_36 = r.rolling(36).mean().fillna(0.0).values

    up = r.clip(lower=0).rolling(14).mean()
    down = (-r.clip(upper=0)).rolling(14).mean()
    rsi = 100.0 * up / (up + down + 1e-9)
    rsi = ((rsi - 50.0) / 50.0).fillna(0.0).values

    smooth = r.ewm(span=10).mean().values

    return (
        returns.astype(np.float32),
        vol_12.astype(np.float32),
        vol_36.astype(np.float32),
        trend_36.astype(np.float32),
        rsi.astype(np.float32),
        smooth.astype(np.float32),
    )


# ================== Mamba (toy) ==================

class MambaBlock(nn.Module):
    def __init__(self, d_model: int, d_state: int, dt_rank: str = "auto"):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        if dt_rank == "auto":
            dt_rank = d_model // 16

        self.in_proj = nn.Linear(d_model, d_model * 2, bias=False)
        self.conv1d = nn.Conv1d(
            in_channels=d_model,
            out_channels=d_model,
            kernel_size=4,
            padding=3,
            groups=d_model,
            bias=True
        )
        self.x_proj = nn.Linear(d_model, dt_rank + d_state * 2, bias=False)
        self.dt_proj = nn.Linear(dt_rank, d_model, bias=True)

        A = np.arange(1, d_state + 1, dtype=np.float32)
        self.register_buffer("A_log", torch.log(torch.tensor(A)))
        self.register_buffer("D", torch.ones(d_model))

        self.out_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        B, L, D = x.shape
        x_proj = self.in_proj(x)
        x_a, x_b = x_proj.chunk(2, dim=-1)
        x_a = F.silu(x_a)

        x_a = x_a.transpose(1, 2)
        x_a = self.conv1d(x_a)[:, :, :L]
        x_a = x_a.transpose(1, 2)

        _A = -torch.exp(self.A_log)  # placeholder
        y = x_a * torch.sigmoid(x_b)

        y = self.out_proj(y)
        return y


class MambaForecaster(nn.Module):
    def __init__(self, input_dim: int = 6, d_model: int = cfg.MAMBA_D_MODEL):
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.blocks = nn.ModuleList([MambaBlock(d_model, cfg.MAMBA_D_STATE) for _ in range(cfg.MAMBA_LAYERS)])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))

    def forward(self, x):
        x = self.embedding(x)
        for b in self.blocks:
            x = b(x) + x
        x = self.norm(x)
        x = x[:, -1, :]
        return self.head(x)


# ================== Flash Attention 2 wrapper ==================

def flash_attention_2(q, k, v, causal=False, dropout_p=0.0, training=False):
    if hasattr(F, "scaled_dot_product_attention"):
        return F.scaled_dot_product_attention(
            q, k, v,
            dropout_p=dropout_p if training else 0.0,
            is_causal=causal
        )
    # fallback
    scale = q.size(-1) ** -0.5
    scores = torch.matmul(q, k.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1)
    if dropout_p > 0 and training:
        attn = F.dropout(attn, p=dropout_p)
    return torch.matmul(attn, v)


class FlashAttention(nn.Module):
    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.dropout = dropout
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, L, D = x.shape
        q = self.q_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)
        k = self.k_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)
        v = self.v_proj(x).reshape(B, L, self.nhead, D // self.nhead).transpose(1, 2)

        out = flash_attention_2(q, k, v, dropout_p=self.dropout, training=self.training)
        out = out.transpose(1, 2).reshape(B, L, D)
        return self.out_proj(out)


# ================== LoRA ==================

class LoRA_Linear(nn.Module):
    def __init__(self, in_features: int, out_features: int, r: int, alpha: float, dropout: float = 0.05):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)
        self.bias = nn.Parameter(torch.zeros(out_features))

        self.lora_a = nn.Parameter(torch.randn(in_features, r) * 0.01)
        self.lora_b = nn.Parameter(torch.zeros(r, out_features))
        self.dropout = nn.Dropout(dropout)

        self.scaling = alpha / r

    def forward(self, x):
        # base
        base = F.linear(x, self.weight, self.bias)
        # lora
        lora = (self.dropout(x) @ self.lora_a @ self.lora_b) * self.scaling
        return base + lora


def inject_lora(module: nn.Module, r: int, alpha: float, target_linear: Tuple[type, ...] = (nn.Linear,)):
    """
    Recursively replace nn.Linear with LoRA_Linear
    """
    for name, child in list(module.named_children()):
        if isinstance(child, target_linear):
            new = LoRA_Linear(child.in_features, child.out_features, r=r, alpha=alpha)
            # copy base weights if possible
            with torch.no_grad():
                new.weight.copy_(child.weight.data)
                if child.bias is not None:
                    new.bias.copy_(child.bias.data)
            setattr(module, name, new)
        else:
            inject_lora(child, r=r, alpha=alpha, target_linear=target_linear)


# ================== Temporal Fusion Transformer (simplified) ==================

class TemporalFusionTransformer(nn.Module):
    """
    Simplified TFT-like encoder/decoder + latent attention.
    - Use FlashAttention for latent attention when enabled.
    """
    def __init__(self, input_dim: int = 6):
        super().__init__()
        d_model = cfg.TFT_D_MODEL
        nhead = cfg.TFT_NHEAD
        num_layers = cfg.TFT_LAYERS
        dim_ff = cfg.TFT_FF

        self.embed = nn.Sequential(
            nn.Linear(input_dim, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model),
        )

        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=dim_ff,
                dropout=0.2,
                batch_first=True,
                norm_first=True,
            )
            for _ in range(num_layers)
        ])

        self.decoder_layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=dim_ff,
                dropout=0.2,
                batch_first=True,
                norm_first=True,
            )
            for _ in range(num_layers)
        ])

        # latent attention: FlashAttention or standard MHA
        if cfg.USE_FLASH_ATTENTION:
            self.latent_attn = FlashAttention(d_model, nhead, dropout=0.2)
            self._latent_is_flash = True
        else:
            self.latent_attn = nn.MultiheadAttention(d_model, nhead, dropout=0.2, batch_first=True)
            self._latent_is_flash = False

        self.norm = nn.LayerNorm(d_model)
        self.pred_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
        self.unc_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))

    def forward(self, x):
        x = self.embed(x)

        for enc in self.encoder_layers:
            x = enc(x)

        # latent token = last token expanded
        latent = x[:, -1:, :].expand(-1, x.size(1), -1)

        if self._latent_is_flash:
            # FlashAttention expects (B,L,D) and returns (B,L,D)
            x_attn = self.latent_attn(latent)  # self-attn on latent (cheap proxy)
            # mix with context
            x_attn = x_attn + latent
        else:
            x_attn, _ = self.latent_attn(latent, x, x)

        for dec in self.decoder_layers:
            x_attn = dec(x_attn, x)

        out = self.norm(x)
        last = out[:, -1, :]

        pred = self.pred_head(last)
        unc = F.softplus(self.unc_head(last)) + 1e-6
        return pred, unc


class EnsembleForecaster(nn.Module):
    """
    Use N models and return mean prediction and std (uncertainty).
    """
    def __init__(self, n_models: int):
        super().__init__()
        self.models = nn.ModuleList([TemporalFusionTransformer(input_dim=6) for _ in range(n_models)])

        if cfg.USE_LORA:
            for m in self.models:
                inject_lora(m, r=cfg.LORA_RANK, alpha=cfg.LORA_ALPHA)

    def forward(self, x, return_uncertainty: bool = True):
        preds = []
        uncs = []
        for m in self.models:
            p, u = m(x)
            preds.append(p)
            uncs.append(u)

        preds = torch.stack(preds, dim=0)  # (E,B,H)
        uncs = torch.stack(uncs, dim=0)    # (E,B,H)

        mean_pred = preds.mean(dim=0)
        # uncertainty: ensemble std + aleatoric mean
        ens_std = preds.std(dim=0)
        alea = uncs.mean(dim=0)
        total_unc = ens_std + alea

        if return_uncertainty:
            return mean_pred, total_unc
        return mean_pred


# ================== Dataset builder ==================

def make_supervised_dataset(prices: np.ndarray, seq_len: int):
    returns, vol_12, vol_36, trend_36, rsi, smooth = build_returns_and_tech(prices)

    feat = np.stack([returns, vol_12, vol_36, trend_36, rsi, smooth], axis=1).astype(np.float32)
    T = feat.shape[0]
    horizon_max = max(cfg.FORECAST_HORIZONS)

    X_list = []
    y_list = []
    for t in range(seq_len, T - horizon_max):
        X_list.append(feat[t - seq_len: t])
        y_list.append([returns[t + h - 1] for h in cfg.FORECAST_HORIZONS])

    X = torch.tensor(np.array(X_list, dtype=np.float32), device=device)
    y = torch.tensor(np.array(y_list, dtype=np.float32), device=device)
    return X, y, returns, vol_12, vol_36, trend_36, rsi


# ================== Forecaster training ==================

def train_forecasters_v4(prices: np.ndarray):
    print("\n[Training] Mamba + TFT(Ensemble)")

    X, y, returns, vol_12, vol_36, trend_36, rsi = make_supervised_dataset(prices, cfg.MAMBA_SEQ_LEN)
    N = len(X)
    train_size = int(N * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    ds = torch.utils.data.TensorDataset(X_train, y_train)
    loader = torch.utils.data.DataLoader(ds, batch_size=cfg.MAMBA_BATCH, shuffle=True)

    scaler = GradScaler(enabled=(cfg.USE_FP16 and device.type == "cuda"))

    # ---- Mamba ----
    mamba = MambaForecaster(input_dim=X.shape[2]).to(device)
    opt_m = optim.AdamW(mamba.parameters(), lr=cfg.MAMBA_LR, weight_decay=1e-4)
    sch_m = CosineAnnealingWarmRestarts(opt_m, T_0=5, T_mult=2)
    crit = nn.MSELoss()

    for ep in range(1, cfg.MAMBA_EPOCHS + 1):
        mamba.train()
        loss_sum = 0.0
        for xb, yb in loader:
            opt_m.zero_grad(set_to_none=True)
            if scaler.is_enabled():
                with autocast(dtype=torch.float16):
                    pred = mamba(xb)
                    loss = crit(pred, yb)
                scaler.scale(loss).backward()
                scaler.step(opt_m)
                scaler.update()
            else:
                pred = mamba(xb)
                loss = crit(pred, yb)
                loss.backward()
                opt_m.step()
            loss_sum += loss.item()
        sch_m.step()

        mamba.eval()
        with torch.no_grad():
            val_pred = mamba(X_val)
            val_loss = crit(val_pred, y_val).item()
        print(f"[Mamba] Ep {ep}/{cfg.MAMBA_EPOCHS} Train={loss_sum/len(loader):.6e} Val={val_loss:.6e}")

    # ---- TFT Ensemble ----
    if cfg.USE_ENSEMBLE:
        tft_ens = EnsembleForecaster(cfg.N_ENSEMBLE_MODELS).to(device)
    else:
        tft_ens = EnsembleForecaster(1).to(device)

    opt_t = optim.AdamW(tft_ens.parameters(), lr=cfg.TFT_LR, weight_decay=1e-4)
    sch_t = CosineAnnealingWarmRestarts(opt_t, T_0=5, T_mult=2)

    ds_tft = torch.utils.data.TensorDataset(X_train, y_train)
    loader_tft = torch.utils.data.DataLoader(ds_tft, batch_size=cfg.TFT_BATCH, shuffle=True)

    for ep in range(1, cfg.TFT_EPOCHS + 1):
        tft_ens.train()
        loss_sum = 0.0
        for xb, yb in loader_tft:
            opt_t.zero_grad(set_to_none=True)
            if scaler.is_enabled():
                with autocast(dtype=torch.float16):
                    pred, unc = tft_ens(xb, return_uncertainty=True)
                    nll = torch.mean((pred - yb) ** 2 / (unc + 1e-6))
                    reg = torch.mean(torch.log(unc + 1e-6))
                    loss = nll + 0.1 * reg
                scaler.scale(loss).backward()
                scaler.step(opt_t)
                scaler.update()
            else:
                pred, unc = tft_ens(xb, return_uncertainty=True)
                nll = torch.mean((pred - yb) ** 2 / (unc + 1e-6))
                reg = torch.mean(torch.log(unc + 1e-6))
                loss = nll + 0.1 * reg
                loss.backward()
                opt_t.step()
            loss_sum += loss.item()
        sch_t.step()

        tft_ens.eval()
        with torch.no_grad():
            val_pred, val_unc = tft_ens(X_val, return_uncertainty=True)
            val_loss = torch.mean((val_pred - y_val) ** 2 / (val_unc + 1e-6)).item()
        print(f"[TFT-ENS] Ep {ep}/{cfg.TFT_EPOCHS} Train={loss_sum/len(loader_tft):.6e} Val={val_loss:.6e}")

    return mamba, tft_ens, (returns, vol_12, vol_36, trend_36, rsi)


# ================== Feeders ==================

def build_feeders_v4(prices: np.ndarray, mamba_model: nn.Module, tft_ens: nn.Module):
    returns, vol_12, vol_36, trend_36, rsi, smooth = build_returns_and_tech(prices)
    feat = np.stack([returns, vol_12, vol_36, trend_36, rsi, smooth], axis=1).astype(np.float32)

    def _slice_x(t: int):
        end = t + 1
        start = end - cfg.MAMBA_SEQ_LEN
        x = feat[start:end]
        return torch.tensor(x, dtype=torch.float32, device=device).unsqueeze(0)

    def mamba_feeder(t: int):
        if t < cfg.MAMBA_SEQ_LEN:
            return np.zeros(len(cfg.FORECAST_HORIZONS), dtype=np.float32)
        x_t = _slice_x(t)
        mamba_model.eval()
        with torch.no_grad():
            pred = mamba_model(x_t).squeeze(0).detach().cpu().numpy()
        return pred.astype(np.float32)

    def tft_feeder(t: int):
        if t < cfg.MAMBA_SEQ_LEN:
            return np.zeros(len(cfg.FORECAST_HORIZONS), dtype=np.float32), np.zeros(len(cfg.FORECAST_HORIZONS), dtype=np.float32)
        x_t = _slice_x(t)
        tft_ens.eval()
        with torch.no_grad():
            pred, unc = tft_ens(x_t, return_uncertainty=True)
        return pred.squeeze(0).detach().cpu().numpy().astype(np.float32), unc.squeeze(0).detach().cpu().numpy().astype(np.float32)

    def regime_feeder(t: int):
        if t < cfg.MAMBA_SEQ_LEN:
            return (np.ones(3, dtype=np.float32) / 3.0)
        v = vol_36[t]
        tr = trend_36[t]
        # simple probabilistic regime
        if abs(tr) > cfg.TREND_THRESHOLD and v < 3 * cfg.TREND_THRESHOLD:
            return np.array([0.1, 0.8, 0.1], dtype=np.float32)  # Trend
        if v > 3 * cfg.TREND_THRESHOLD:
            return np.array([0.1, 0.1, 0.8], dtype=np.float32)  # HighVol
        return np.array([0.8, 0.1, 0.1], dtype=np.float32)      # Range

    return mamba_feeder, tft_feeder, regime_feeder, returns, vol_12, vol_36, trend_36, rsi


# ================== Env ==================

class HybridEnvV4:
    def __init__(self, prices: np.ndarray, feeders):
        self.mamba_feeder, self.tft_feeder, self.regime_feeder, returns, vol_12, vol_36, trend_36, rsi = feeders
        self.prices = torch.tensor(prices, dtype=torch.float32, device=device)
        self.returns = torch.tensor(returns, dtype=torch.float32, device=device)
        self.vol_12 = torch.tensor(vol_12, dtype=torch.float32, device=device)
        self.vol_36 = torch.tensor(vol_36, dtype=torch.float32, device=device)
        self.trend_36 = torch.tensor(trend_36, dtype=torch.float32, device=device)
        self.rsi = torch.tensor(rsi, dtype=torch.float32, device=device)

        self.reset_idx = cfg.STATE_RET_LEN
        self.max_t = len(self.returns) - 1
        self.position = 0
        self.t = None

        # Fusion: mamba_pred + tft_pred + regime(3) -> fused_pred
        self.fusion = nn.Linear(len(cfg.FORECAST_HORIZONS) * 2 + 3, len(cfg.FORECAST_HORIZONS)).to(device)

    @property
    def state_dim(self):
        # ret_window + vol + trend + rsi + regime(3) + fused_pred(H) + unc(H) + pos
        H = len(cfg.FORECAST_HORIZONS)
        return cfg.STATE_RET_LEN + 1 + 1 + 1 + 3 + H + H + 1

    def reset(self):
        self.t = self.reset_idx
        self.position = 0
        return self._get_state()

    def _get_state(self):
        start = self.t - cfg.STATE_RET_LEN
        ret_window = self.returns[start:self.t].detach().cpu().numpy()

        vol = float(self.vol_12[self.t].item())
        trend = float(self.trend_36[self.t].item())
        rsi_val = float(self.rsi[self.t].item())

        regime = self.regime_feeder(self.t)
        m_pred = self.mamba_feeder(self.t)
        t_pred, t_unc = self.tft_feeder(self.t)

        comb = torch.tensor(np.concatenate([m_pred, t_pred, regime], axis=0), dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            fused = self.fusion(comb).squeeze(0).detach().cpu().numpy()

        pos_scaled = self.position / cfg.MAX_POSITION

        # uncertainty: combine tft uncertainty + disagreement(mamba vs tft)
        disagree = np.abs(m_pred - t_pred).astype(np.float32)
        unc = (t_unc + disagree).astype(np.float32)

        state = np.concatenate([
            ret_window,
            np.array([vol, trend, rsi_val], dtype=np.float32),
            regime.astype(np.float32),
            fused.astype(np.float32),
            unc.astype(np.float32),
            np.array([pos_scaled], dtype=np.float32),
        ])
        return state.astype(np.float32)

    def step(self, action_idx: int):
        action_to_pos = np.array([-5, -3, -1, 0, 1, 3, 5], dtype=np.int32)
        new_pos = int(action_to_pos[action_idx])

        prev_pos = self.position
        self.position = new_pos
        pos_change = abs(self.position - prev_pos)

        r = float(self.returns[self.t].item())
        price = float(self.prices[self.t].item())

        pnl = self.position * r
        volume_cost = pos_change * cfg.TRANSACTION_COST

        spread_return = (cfg.SPREAD_PIPS * cfg.PIP_VALUE_JPY) / max(price, 1e-9)
        slippage_return = (cfg.SLIPPAGE_PIPS * cfg.PIP_VALUE_JPY) / max(price, 1e-9)

        if pos_change > 0:
            spread_slip_cost = abs(self.position) * (spread_return + slippage_return)
        else:
            spread_slip_cost = 0.0

        cost = volume_cost + spread_slip_cost
        reward = pnl - cost

        if reward < 0:
            reward *= cfg.LOSS_FACTOR

        trend = float(self.trend_36[self.t].item())
        if abs(trend) > cfg.TREND_THRESHOLD:
            reward *= cfg.TREND_BOOST

        # guide reward by forecasts
        m_pred = self.mamba_feeder(self.t)
        t_pred, _ = self.tft_feeder(self.t)
        reward += 0.3 * float(m_pred[0]) * self.position
        reward += 0.3 * float(t_pred[0]) * self.position

        self.t += 1
        done = self.t >= self.max_t
        next_state = self._get_state() if not done else None
        return next_state, float(reward), done, {}


# ================== ActorCritic (uncertainty aware) ==================

class ActorCriticV4(nn.Module):
    def __init__(self, state_dim: int, n_actions: int):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.policy_head = nn.Linear(256, n_actions)
        self.value_head = nn.Linear(256, 1)

    def forward(self, x):
        h = self.shared(x)
        logits = self.policy_head(h)
        value = self.value_head(h).squeeze(-1)
        return logits, value


# ================== PPO + (optional) DPO ==================

def collect_trajectory(env: HybridEnvV4, model: ActorCriticV4, steps_per_ep: int):
    model.eval()
    state = env.reset()

    states, actions, rewards, dones, logps, values = [], [], [], [], [], []

    for _ in range(steps_per_ep):
        s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            logits, value = model(s_t)
            probs = torch.softmax(logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
            logp = dist.log_prob(action)

        next_state, reward, done, _ = env.step(int(action.item()))

        states.append(state)
        actions.append(action.item())
        rewards.append(reward)
        dones.append(done)
        logps.append(logp.item())
        values.append(value.item())

        state = next_state
        if done:
            break

    if not dones[-1] and state is not None:
        with torch.no_grad():
            s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            _, last_value = model(s_t)
            last_value = last_value.item()
    else:
        last_value = 0.0

    return (
        np.array(states, dtype=np.float32),
        np.array(actions, dtype=np.int64),
        np.array(rewards, dtype=np.float32),
        np.array(dones, dtype=np.bool_),
        np.array(logps, dtype=np.float32),
        np.array(values, dtype=np.float32),
        float(last_value),
    )


def compute_gae(rewards, dones, values, last_value):
    T = len(rewards)
    adv = np.zeros(T, dtype=np.float32)
    gae = 0.0
    for t in reversed(range(T)):
        mask = 1.0 - float(dones[t])
        next_val = last_value if t == T - 1 else values[t + 1]
        delta = rewards[t] + cfg.GAMMA * next_val * mask - values[t]
        gae = delta + cfg.GAMMA * cfg.LAMBDA_GAE * mask * gae
        adv[t] = gae
    ret = adv + np.array(values, dtype=np.float32)
    return adv, ret


def dpo_step(model: ActorCriticV4, optimizer: optim.Optimizer, states_t: torch.Tensor, beta: float):
    """
    ‚úÖ Á∞°ÊòìDPOÔºàDirect Preference OptimizationÔºâ
    - ‚ÄúËâØ„ÅÑË°åÂãï‚Äù = model„ÅåÈÅ∏„Çì„Å† argmax
    - ‚ÄúÊÇ™„ÅÑË°åÂãï‚Äù = „É©„É≥„ÉÄ„É†„Å´Âà•Ë°åÂãï
    ‚ÄªÊú¨Êù•„ÅØ‰∫∫Èñì/ÊØîËºÉÂô®„ÅÆÂ•Ω„Åø„Éá„Éº„Çø„ÅåÂøÖË¶Å„Å†„Åå„ÄÅ„Åì„Åì„Åß„ÅØËªΩÈáè„Å´ ‚Äúself-preference‚Äù „Åß‰ª£Áî®
    """
    model.train()
    with torch.no_grad():
        logits, _ = model(states_t)
        good = torch.argmax(logits, dim=-1)
        bad = torch.randint_like(good, low=0, high=logits.size(-1))
        bad = torch.where(bad == good, (bad + 1) % logits.size(-1), bad)

    logits, _ = model(states_t)
    logp = torch.log_softmax(logits, dim=-1)
    logp_good = logp.gather(1, good.unsqueeze(1)).squeeze(1)
    logp_bad  = logp.gather(1, bad.unsqueeze(1)).squeeze(1)

    # DPO objective
    # maximize log(sigmoid(beta*(logp_good - logp_bad)))
    loss = -torch.mean(F.logsigmoid(beta * (logp_good - logp_bad)))

    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    return float(loss.item())


def train_with_config_v4(envs: List[HybridEnvV4], config: Dict) -> Tuple[float, ActorCriticV4]:
    cfg.LOSS_FACTOR = config["loss_factor"]
    cfg.TREND_BOOST = config["trend_boost"]
    lr = config["lr"]

    model = ActorCriticV4(envs[0].state_dim, cfg.N_ACTIONS).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scaler = GradScaler(enabled=(cfg.USE_FP16 and device.type == "cuda"))

    episode_rewards = []
    total_episodes = cfg.EPISODES_PER_PAIR * len(envs)

    for ep in range(1, total_episodes + 1):
        env = np.random.choice(envs)

        states, actions, rewards, dones, old_logp, values, last_val = collect_trajectory(env, model, cfg.STEPS_PER_EP)
        adv, ret = compute_gae(rewards, dones, values, last_val)
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)

        states_t = torch.tensor(states, dtype=torch.float32, device=device)
        actions_t = torch.tensor(actions, dtype=torch.long, device=device)
        old_logp_t = torch.tensor(old_logp, dtype=torch.float32, device=device)
        adv_t = torch.tensor(adv, dtype=torch.float32, device=device)
        ret_t = torch.tensor(ret, dtype=torch.float32, device=device)

        dataset_size = states_t.size(0)
        ep_reward = float(np.sum(rewards))
        episode_rewards.append(ep_reward)

        model.train()
        for _ in range(cfg.EPOCHS_PPO):
            idx = np.random.permutation(dataset_size)
            for start in range(0, dataset_size, cfg.MINI_BATCH):
                end = min(start + cfg.MINI_BATCH, dataset_size)
                mb_idx = idx[start:end]
                mb_s = states_t[mb_idx]
                mb_a = actions_t[mb_idx]
                mb_old = old_logp_t[mb_idx]
                mb_adv = adv_t[mb_idx]
                mb_ret = ret_t[mb_idx]

                if scaler.is_enabled():
                    with autocast(dtype=torch.float16):
                        logits, values_pred = model(mb_s)
                        probs = torch.softmax(logits, dim=-1)
                        dist = torch.distributions.Categorical(probs)
                        logp = dist.log_prob(mb_a)

                        ratio = torch.exp(logp - mb_old)
                        surr1 = ratio * mb_adv
                        surr2 = torch.clamp(ratio, 1.0 - cfg.CLIP_EPS, 1.0 + cfg.CLIP_EPS) * mb_adv
                        policy_loss = -torch.min(surr1, surr2).mean()
                        value_loss = F.mse_loss(values_pred, mb_ret)
                        entropy = dist.entropy().mean()

                        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy

                    optimizer.zero_grad(set_to_none=True)
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    logits, values_pred = model(mb_s)
                    probs = torch.softmax(logits, dim=-1)
                    dist = torch.distributions.Categorical(probs)
                    logp = dist.log_prob(mb_a)

                    ratio = torch.exp(logp - mb_old)
                    surr1 = ratio * mb_adv
                    surr2 = torch.clamp(ratio, 1.0 - cfg.CLIP_EPS, 1.0 + cfg.CLIP_EPS) * mb_adv
                    policy_loss = -torch.min(surr1, surr2).mean()
                    value_loss = F.mse_loss(values_pred, mb_ret)
                    entropy = dist.entropy().mean()

                    loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
                    optimizer.zero_grad(set_to_none=True)
                    loss.backward()
                    optimizer.step()

        # ‚úÖ DPO: PPOÂæå„Å´ËªΩÈáè„ÅßËøΩÂä†ÔºàÊú¨Êù•„ÅØÂ§ñÈÉ® preference „Éá„Éº„Çø„ÅåÁêÜÊÉ≥Ôºâ
        if cfg.USE_DPO:
            dpo_opt = optim.Adam(model.parameters(), lr=lr * cfg.DPO_LR_MULT)
            # states„Åã„Çâ‰∏ÄÈÉ®„Çµ„É≥„Éó„É´„Åó„Å¶ÂÆüÊñΩ
            if dataset_size > 8:
                take = min(cfg.DPO_NUM_PAIRS, dataset_size)
                pick = torch.tensor(np.random.choice(dataset_size, take, replace=False), device=device)
                for _ in range(cfg.DPO_STEPS_PER_EP):
                    dpo_loss = dpo_step(model, dpo_opt, states_t[pick], beta=cfg.DPO_BETA)

        if ep % 10 == 0:
            print(f"[PPO/DPO v4] Ep {ep}/{total_episodes} reward={ep_reward:.4f}")

    avg_last = float(np.mean(episode_rewards[-len(envs):]))
    return avg_last, model


# ================== Meta search ==================

def meta_search_v4(envs: List[HybridEnvV4], trials: int = 20) -> Tuple[Dict, ActorCriticV4]:
    best_cfg = None
    best_score = -1e9
    best_model = None

    print("\n" + "=" * 70)
    print("üîç META SEARCH v4 (Random Search)")
    print("=" * 70)

    for i in range(trials):
        trial_cfg = {
            "lr": float(10 ** np.random.uniform(-4.0, -2.8)),
            "loss_factor": float(np.random.uniform(0.9, 1.8)),
            "trend_boost": float(np.random.uniform(1.2, 3.5)),
        }

        score, model = train_with_config_v4(envs, trial_cfg)
        print(f"[Meta] Trial {i+1}/{trials} cfg={trial_cfg} score={score:.4f}")

        if score > best_score:
            best_score = score
            best_cfg = trial_cfg
            best_model = model

    print("=" * 70)
    print(f"‚úÖ Best Config: {best_cfg}")
    print(f"‚úÖ Best Score:  {best_score:.4f}")
    print("=" * 70)
    return best_cfg, best_model


# ================== Quantization for inference ==================

def maybe_quantize_for_inference(model: nn.Module) -> nn.Module:
    """
    ‚úÖ INT8 dynamic quantization (CPUÊé®Ë´ñÂêë„Åë)
    - CUDA‰∏ä„Åß„ÅØÂü∫Êú¨„É°„É™„ÉÉ„Éà„ÅåËñÑ„ÅÑ„ÅÆ„Åß„ÄÅCPUÊé®Ë´ñ„Å´Âàá„ÇäÊõø„Åà„ÇãÂ†¥Âêà„Å´‰Ωø„ÅÜ„ÅÆ„ÅåÂêâ
    """
    if not cfg.USE_QUANTIZATION:
        return model

    # dynamic quant works on CPU
    model_cpu = model.to("cpu").eval()
    q = torch.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)
    return q


# ================== Simulation ==================

def run_simulation_v4(env: HybridEnvV4, model: nn.Module, steps: int = 600, log_interval: int = 60):
    # model may be quantized (CPU). move state to CPU if needed.
    use_cpu = next(model.parameters()).device.type == "cpu"

    model.eval()
    state = env.reset()
    equity = 1.0
    eq_curve = [equity]

    print("\n" + "=" * 70)
    print("üöÄ INFERENCE SIMULATION v4 (SOTA actually used)")
    print("=" * 70)

    for t in range(steps):
        if use_cpu:
            s_t = torch.tensor(state, dtype=torch.float32, device="cpu").unsqueeze(0)
        else:
            s_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

        with torch.no_grad():
            logits, _ = model(s_t)
            action = int(torch.argmax(logits, dim=-1).item())

        next_state, reward, done, _ = env.step(action)
        equity *= (1.0 + reward)
        eq_curve.append(equity)

        if t % log_interval == 0 or done:
            print(f"[Sim] t={t:4d} action={action} pos={env.position:2d} reward={reward: .6f} equity={equity: .4f}")

        state = next_state
        if done:
            break

    plt.figure(figsize=(14, 5))
    plt.plot(eq_curve, linewidth=2)
    plt.title("v4 Simulation - Equity Curve (SOTA actually used)")
    plt.xlabel("Steps")
    plt.ylabel("Equity Multiplier")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig("simulation_v4_sota_actually_used.png", dpi=150)
    print("‚úÖ Saved: simulation_v4_sota_actually_used.png")
    print(f"Final Equity: {equity:.4f} | Total Return: {(equity - 1.0) * 100:.2f}%")
    print("=" * 70)


# ================== Main ==================

def main_v4():
    envs = []

    for csv in cfg.PAIR_CSV_LIST:
        df = load_close_series(csv)
        prices = df["close"].values

        mamba_model, tft_ens, tech_pack = train_forecasters_v4(prices)
        feeders = build_feeders_v4(prices, mamba_model, tft_ens)
        env = HybridEnvV4(prices, feeders)
        envs.append(env)

    best_cfg, best_model = meta_search_v4(envs, trials=cfg.META_TRIALS)

    # ‚úÖ quantize for inference (CPU)
    q_model = maybe_quantize_for_inference(best_model)

    # simulate
    run_simulation_v4(envs[0], q_model, steps=600, log_interval=60)

    print("\n‚úÖ All done!")


if __name__ == "__main__":
    main_v4()
