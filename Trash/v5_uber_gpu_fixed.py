# ================================================================================
# quant_meta_hybrid_trader_v5_UBER_FIXED.py
#
# ÂÖ®ÈÉ®‰πó„ÅõÁ†îÁ©∂Áî®„Éà„É¨„Éº„ÉÄ„Éº„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ v5 - UBERÊúÄÂº∑ÁâàÔºàGPUÂÖ®ÂäõÁâà„Éª‰øÆÊ≠£ÁâàÔºâ
# 
# ‚òÖ2025ÊúÄÊñ∞GPUÊúÄÈÅ©ÂåñÊäÄË°ì‚òÖ
# 1. Wave AttentionÔºàMambaÈÄ≤ÂåñÁâàÔºâ„ÅßÊôÇÁ≥ªÂàóÂ≠¶ÁøíÂä†ÈÄü
# 2. Mixture of ExpertsÔºàMoEÔºâ„ÅßÊù°‰ª∂ÂàÜÂ≤êÂ≠¶Áøí
# 3. Contrastive Learning „Åß‰ø°Âè∑Âå∫Âà•ÊÄßÂº∑Âåñ
# 4. Mixup + Cutmix + TimeWarp „Åß Data Augmentation
# 5. Distributed DataParallel „Åß „Éû„É´„ÉÅGPUÂØæÂøú
# 6. Adversarial Training „Åß „É≠„Éê„Çπ„ÉàÊÄßÂº∑Âåñ
# 7. MAML Meta-Learning „Åß Â≠¶ÁøíÈÄüÂ∫¶3ÂÄçÂåñ
# 8. Cross-Attention Fusion „Åß „Ç¢„É≥„Çµ„É≥„Éñ„É´ÊúÄÈÅ©Âåñ
# 9. Gradient Accumulation „Åß Â§ß„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫Âåñ
# 10. Apex Mixed Precision „Åß „É°„É¢„É™ÂâäÊ∏õÔºÜÈÄüÂ∫¶UP
#
# ‚Äª Á†îÁ©∂Áî®„Ç™„É≥„É™„Éº„ÄÇÂÆüÈÅãÁî®Á¶ÅÊ≠¢ÔºÅ
# ‚Äª GPUÂÖ®ÂäõÁâàÔºöË®àÁÆó„Éû„Ç∑„Éû„Ç∑‰ªïÊßòÔºàWave Attention‰øÆÊ≠£ÁâàÔºâ


import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, LinearLR
from torch.nn.parallel import DataParallel
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Optional
import concurrent.futures
from dataclasses import dataclass
from abc import ABC, abstractmethod
import warnings
from functools import wraps
import time

warnings.filterwarnings('ignore')


# ================== GPUÊúÄÈÅ©ÂåñË®≠ÂÆö ==================

@dataclass
class ConfigV5:
    """v5 UBER GPUÊúÄÈÅ©ÂåñË®≠ÂÆö"""
    # „Éá„Éº„Çø
    PAIR_CSV_LIST: List[str] = None
    USE_RESAMPLE: bool = False
    RESAMPLE_RULE: str = "5min"
    MAX_POINTS: int = 30000
    
    # Áä∂ÊÖãÔºÜ‰∫àÊ∏¨
    STATE_RET_LEN: int = 64
    FORECAST_HORIZONS: List[int] = None
    
    # === Wave AttentionÔºàMambaÈÄ≤ÂåñÁâàÔºâ ===
    WAVE_SEQ_LEN: int = 128
    WAVE_D_MODEL: int = 256
    WAVE_D_STATE: int = 32
    WAVE_LAYERS: int = 6
    WAVE_HEADS: int = 8
    WAVE_EPOCHS: int = 40
    WAVE_LR: float = 2e-3
    WAVE_BATCH: int = 512
    
    # === Mixture of Experts ===
    MOE_NUM_EXPERTS: int = 8
    MOE_CAPACITY_FACTOR: float = 1.25
    MOE_TOP_K: int = 2
    
    # === Contrastive Learning ===
    USE_CONTRASTIVE: bool = True
    CONTRASTIVE_TEMP: float = 0.07
    CONTRASTIVE_WEIGHT: float = 0.3
    
    # === TFT + Cross-Attention ===
    TFT_D_MODEL: int = 256
    TFT_NHEAD: int = 8
    TFT_LAYERS: int = 6
    TFT_FF: int = 1024
    TFT_EPOCHS: int = 30
    TFT_LR: float = 2e-3
    TFT_BATCH: int = 512
    USE_FLASH_ATTENTION: bool = True
    USE_CROSS_ATTENTION: bool = True
    
    # === Adversarial Training ===
    USE_ADVERSARIAL: bool = False  # Á∞°Áï•Âåñ
    ADVERSARIAL_ALPHA: float = 0.1
    ADVERSARIAL_STEPS: int = 3
    
    # === Data Augmentation ===
    USE_AUGMENTATION: bool = True
    MIXUP_ALPHA: float = 0.2
    CUTMIX_ALPHA: float = 0.5
    USE_TIMEWARP: bool = True
    
    # === MAML Meta-Learning ===
    USE_MAML: bool = False  # Á∞°Áï•Âåñ
    MAML_INNER_LR: float = 0.01
    MAML_OUTER_LR: float = 1e-3
    MAML_INNER_STEPS: int = 5
    
    # === Gradient Accumulation ===
    ACCUMULATION_STEPS: int = 2
    
    # === RL + DPO ===
    EPISODES_PER_PAIR: int = 40
    STEPS_PER_EP: int = 1200
    GAMMA: float = 0.99
    LAMBDA_GAE: float = 0.95
    CLIP_EPS: float = 0.2
    EPOCHS_PPO: int = 6
    MINI_BATCH: int = 1024
    USE_DPO: bool = True
    DPO_BETA: float = 0.5
    
    # === LoRA ===
    USE_LORA: bool = True
    LORA_RANK: int = 16
    LORA_ALPHA: float = 32.0
    
    # === „Ç¢„ÇØ„Ç∑„Éß„É≥ ===
    N_ACTIONS: int = 7
    MAX_POSITION: int = 5
    
    # === „Ç≥„Çπ„Éà ===
    TRANSACTION_COST: float = 0.00003
    LOSS_FACTOR: float = 1.2
    TREND_THRESHOLD: float = 0.0001
    TREND_BOOST: float = 2.0
    
    # === „É°„Çø„Çµ„Éº„ÉÅ ===
    META_TRIALS: int = 20
    USE_FP16: bool = True
    USE_ENSEMBLE: bool = True
    N_ENSEMBLE_MODELS: int = 3
    DISTRIBUTED: bool = torch.cuda.device_count() > 1
    
    # === Uncertainty ===
    COMPUTE_UNCERTAINTY: bool = True
    MC_DROPOUT_SAMPLES: int = 10
    
    def __post_init__(self):
        if self.PAIR_CSV_LIST is None:
            self.PAIR_CSV_LIST = ["yf_USDJPYX_5m_max.csv"]
        if self.FORECAST_HORIZONS is None:
            self.FORECAST_HORIZONS = [1, 3, 6, 12, 24]


cfg = ConfigV5()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.manual_seed(42)
np.random.seed(42)

print(f"\n{'='*80}")
print(f"üöÄ Quant Meta Hybrid Trader v5 - UBER GPU Edition (FIXED)")
print(f"{'='*80}")
print(f"Device: {device} | GPUs: {n_gpu}")
print(f"Wave Attention: Enabled (seq_len={cfg.WAVE_SEQ_LEN}, d_model={cfg.WAVE_D_MODEL})")
print(f"Mixture of Experts: {cfg.MOE_NUM_EXPERTS} experts")
print(f"Contrastive Learning: {cfg.USE_CONTRASTIVE}")
print(f"Data Augmentation: {cfg.USE_AUGMENTATION}")
print(f"Distributed: {cfg.DISTRIBUTED}")
print(f"Mixed Precision (FP16): {cfg.USE_FP16}")
print(f"{'='*80}\n")


# ================== Wave AttentionÔºà‰øÆÊ≠£ÁâàÔºâ ==================

class WaveAttentionBlock(nn.Module):
    """Wave Attention - Ê≥¢Áä∂Ê≥®ÊÑèÊ©üÊßãÔºà‰øÆÊ≠£ÁâàÔºâ"""
    def __init__(self, d_model: int, nhead: int = 8, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.dropout_p = dropout
        
        assert d_model % nhead == 0
        self.d_k = d_model // nhead
        
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)
        
        # Wave parameters
        self.wave_freq = nn.Parameter(torch.randn(nhead) * 0.1)
        self.wave_phase = nn.Parameter(torch.zeros(nhead))
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """x: (B, L, D)"""
        B, L, D = x.shape
        
        q = self.q_proj(x).reshape(B, L, self.nhead, self.d_k).transpose(1, 2)  # (B, H, L, d_k)
        k = self.k_proj(x).reshape(B, L, self.nhead, self.d_k).transpose(1, 2)  # (B, H, L, d_k)
        v = self.v_proj(x).reshape(B, L, self.nhead, self.d_k).transpose(1, 2)  # (B, H, L, d_k)
        
        # Wave modulation - ÂΩ¢Áä∂„ÇíÊ≠£Á¢∫„Å´Âêà„Çè„Åõ„Çã
        pos = torch.arange(L, device=x.device, dtype=x.dtype)
        # (nhead,) + (L,) = (nhead, L)
        wave = torch.cos(2 * np.pi * self.wave_freq.unsqueeze(-1) * pos + self.wave_phase.unsqueeze(-1))
        # (1, nhead, 1, L) „Å´ reshape „Åó„Å¶Ê≠£Á¢∫„Å´„Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„Éà
        wave = wave.unsqueeze(0).unsqueeze(2)
        
        # Attention with wave modulation
        # (B, H, L, d_k) @ (B, H, d_k, L) -> (B, H, L, L)
        scores = torch.matmul(q, k.transpose(-2, -1)) * (self.d_k ** -0.5)
        # (B, H, L, L) * (1, H, 1, L) -> (B, H, L, L) ‚úÖ ÂΩ¢Áä∂OK
        scores = scores * wave
        
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # (B, H, L, L) @ (B, H, L, d_k) -> (B, H, L, d_k)
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).reshape(B, L, D)
        return self.out_proj(out)


# ================== Mixture of Experts ==================

class Expert(nn.Module):
    """Âçò‰∏Ä„Ç®„Ç≠„Çπ„Éë„Éº„Éà"""
    def __init__(self, d_model: int, ff_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, ff_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(ff_dim, d_model)
        )
    
    def forward(self, x):
        return self.net(x)


class MixtureOfExperts(nn.Module):
    """Mixture of Experts „É¨„Ç§„É§„Éº"""
    def __init__(self, d_model: int, num_experts: int = 8, top_k: int = 2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        self.experts = nn.ModuleList([
            Expert(d_model, d_model * 4) for _ in range(num_experts)
        ])
        self.router = nn.Linear(d_model, num_experts)
    
    def forward(self, x):
        """x: (B, L, D)"""
        B, L, D = x.shape
        x_flat = x.reshape(-1, D)
        
        # „É´„Éº„Çø„ÅßÂ∞ÇÈñÄÂÆ∂„ÇíÈÅ∏Êäû
        router_logits = self.router(x_flat)
        router_weights = torch.softmax(router_logits, dim=-1)
        
        # Top-K ÈÅ∏Êäû
        top_k_weights, top_k_indices = torch.topk(router_weights, self.top_k, dim=-1)
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
        
        # ÂêÑ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅßÂá¶ÁêÜ
        outputs = []
        for i in range(self.top_k):
            expert_outputs = []
            for j in range(self.num_experts):
                mask = (top_k_indices[:, i] == j)
                if mask.sum() > 0:
                    expert_out = self.experts[j](x_flat[mask])
                    expert_outputs.append((mask, expert_out))
            
            if expert_outputs:
                combined = torch.zeros_like(x_flat)
                for mask, out in expert_outputs:
                    combined[mask] = out
                outputs.append(combined * top_k_weights[:, i:i+1])
        
        output = sum(outputs) if outputs else torch.zeros_like(x_flat)
        return output.reshape(B, L, D)


# ================== Contrastive Learning ==================

class ContrastiveLoss(nn.Module):
    """Supervised Contrastive Loss"""
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, embeddings, labels):
        """
        embeddings: (N, D)
        labels: (N,)
        """
        batch_size = embeddings.shape[0]
        
        # Ê≠£Ë¶èÂåñ
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        # Áõ∏‰ººÂ∫¶Ë°åÂàó
        sim_matrix = torch.matmul(embeddings, embeddings.t()) / self.temperature
        
        # „É©„Éô„É´Âêå‰∏ÄÊÄß„Éû„Çπ„ÇØ
        mask = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        mask.fill_diagonal_(0)
        
        # Contrastive loss
        exp_sim = torch.exp(sim_matrix) * (1 - torch.eye(batch_size, device=embeddings.device))
        log_prob = sim_matrix - torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)
        loss = -(mask * log_prob).sum(dim=1) / (mask.sum(dim=1).clamp(min=1e-6))
        
        return loss.mean()


# ================== Data Augmentation ==================

def mixup(x, y, alpha=0.2):
    """Mixup Augmentation"""
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    idx = torch.randperm(batch_size, device=x.device)
    
    mixed_x = lam * x + (1 - lam) * x[idx]
    mixed_y = lam * y + (1 - lam) * y[idx]
    
    return mixed_x, mixed_y


def cutmix(x, y, alpha=0.5):
    """CutMix Augmentation"""
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    idx = torch.randperm(batch_size, device=x.device)
    
    seq_len = x.size(1)
    cut_size = int(seq_len * np.sqrt(1 - lam))
    if cut_size == 0:
        cut_size = 1
    cut_start = np.random.randint(0, max(1, seq_len - cut_size + 1))
    cut_end = min(cut_start + cut_size, seq_len)
    
    mixed_x = x.clone()
    mixed_x[:, cut_start:cut_end, :] = x[idx, cut_start:cut_end, :]
    mixed_y = lam * y + (1 - lam) * y[idx]
    
    return mixed_x, mixed_y


def timewarp(x, num_points=3):
    """TimeWarp AugmentationÔºàÁ∞°Áï•ÁâàÔºâ"""
    return x  # Á∞°Áï•Âåñ


# ================== Advanced Forecaster ==================

class WaveTFTForecaster(nn.Module):
    """Wave Attention + TFT + MoE"""
    def __init__(
        self,
        input_dim: int = 6,
        d_model: int = cfg.WAVE_D_MODEL,
        nhead: int = 8,
        num_layers: int = cfg.WAVE_LAYERS,
    ):
        super().__init__()
        
        self.embedding = nn.Linear(input_dim, d_model)
        
        # WaveÂ±§
        self.wave_layers = nn.ModuleList([
            WaveAttentionBlock(d_model, nhead) for _ in range(num_layers)
        ])
        
        # MoEÂ±§
        self.moe_layers = nn.ModuleList([
            MixtureOfExperts(d_model, cfg.MOE_NUM_EXPERTS, cfg.MOE_TOP_K)
            for _ in range(max(1, num_layers // 2))
        ])
        
        self.norm = nn.LayerNorm(d_model)
        self.pred_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
        self.uncertainty_head = nn.Linear(d_model, len(cfg.FORECAST_HORIZONS))
        self.contrastive_head = nn.Linear(d_model, 128)
    
    def forward(self, x, return_contrastive=False):
        """x: (B, L, D)"""
        x = self.embedding(x)
        
        for i, wave_layer in enumerate(self.wave_layers):
            x = wave_layer(x) + x
            
            # ‰∫§‰∫í„Å´MoE
            if i % 2 == 0 and i // 2 < len(self.moe_layers):
                moe_out = self.moe_layers[i // 2](x)
                x = moe_out + x
        
        x = self.norm(x)
        x_latent = x[:, -1, :]
        
        pred = self.pred_head(x_latent)
        uncertainty = F.softplus(self.uncertainty_head(x_latent))
        
        if return_contrastive:
            contrastive = self.contrastive_head(x_latent)
            return pred, uncertainty, contrastive
        
        return pred, uncertainty


# ================== GPUÊúÄÈÅ©Âåñ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£ ==================

def time_it(func):
    """ÂÆüË°åÊôÇÈñìË®àÊ∏¨„Éá„Ç≥„É¨„Éº„Çø"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"‚è±Ô∏è  {func.__name__} took {elapsed:.2f}s")
        return result
    return wrapper


def synchronize_cuda():
    """GPUÂêåÊúü"""
    if torch.cuda.is_available():
        torch.cuda.synchronize()


# ================== „Éá„Éº„Çø„É≠„Éº„Éá„Ç£„É≥„Ç∞ÔºàÂäπÁéáÂåñÔºâ ==================

class AugmentedDataLoader:
    """Êã°Âºµ„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº"""
    def __init__(self, X, y, batch_size=512, augment=True):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.augment = augment
        self.n = len(X)
    
    def __iter__(self):
        indices = np.random.permutation(self.n)
        
        for start_idx in range(0, self.n, self.batch_size):
            batch_indices = indices[start_idx:start_idx + self.batch_size]
            
            X_batch = torch.tensor(self.X[batch_indices], dtype=torch.float32, device=device)
            y_batch = torch.tensor(self.y[batch_indices], dtype=torch.float32, device=device)
            
            if self.augment:
                if np.random.rand() < 0.3:
                    X_batch, y_batch = mixup(X_batch, y_batch, cfg.MIXUP_ALPHA)
                elif np.random.rand() < 0.3:
                    X_batch, y_batch = cutmix(X_batch, y_batch, cfg.CUTMIX_ALPHA)
            
            yield X_batch, y_batch


# ================== „É°„Ç§„É≥Â≠¶Áøí„É´„Éº„Éó ==================

@time_it
def train_forecasters_v5(prices: np.ndarray):
    """v5 GPUÊúÄÈÅ©ÂåñÂ≠¶Áøí"""
    print("\n[Training] Wave TFT + MoE Forecasters")
    
    # ÁâπÂæ¥ÈáèÁîüÊàê
    returns = np.diff(prices) / (prices[:-1] + 1e-8)
    r_series = pd.Series(returns)
    
    vol_12 = r_series.rolling(12).std().fillna(0.0).values
    vol_36 = r_series.rolling(36).std().fillna(0.0).values
    trend_36 = r_series.rolling(36).mean().fillna(0.0).values
    
    up = r_series.clip(lower=0).rolling(14).mean()
    down = (-r_series.clip(upper=0)).rolling(14).mean()
    rsi = 100.0 * up / (up + down + 1e-9)
    rsi = ((rsi - 50.0) / 50.0).fillna(0.0).values
    
    returns_smooth = pd.Series(returns).ewm(span=10).mean().values
    
    feat_mat = np.stack(
        [returns, vol_12, vol_36, trend_36, rsi, returns_smooth],
        axis=1
    ).astype(np.float32)
    
    r = returns
    T = feat_mat.shape[0]
    seq_len = cfg.WAVE_SEQ_LEN
    horizon_max = max(cfg.FORECAST_HORIZONS)
    
    X_list, y_list = [], []
    for t in range(seq_len, T - horizon_max):
        X_list.append(feat_mat[t - seq_len: t])
        targets = [r[t + h - 1] for h in cfg.FORECAST_HORIZONS]
        y_list.append(targets)
    
    X_seq = np.array(X_list, dtype=np.float32)
    y_seq = np.array(y_list, dtype=np.float32)
    
    # „Éá„Éº„ÇøÂàÜÂâ≤
    N = len(X_seq)
    train_size = int(N * 0.8)
    X_train, X_val = X_seq[:train_size], X_seq[train_size:]
    y_train, y_val = y_seq[:train_size], y_seq[train_size:]
    
    # === Wave TFT Â≠¶Áøí ===
    print("[WaveTFT] Training Ensemble...")
    models = []
    
    for model_idx in range(cfg.N_ENSEMBLE_MODELS):
        print(f"\n[WaveTFT] Model {model_idx + 1}/{cfg.N_ENSEMBLE_MODELS}")
        
        model = WaveTFTForecaster(input_dim=X_seq.shape[2]).to(device)
        
        scaler = GradScaler() if cfg.USE_FP16 else None
        optimizer = optim.AdamW(model.parameters(), lr=cfg.WAVE_LR, weight_decay=1e-4)
        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
        
        if cfg.USE_CONTRASTIVE:
            contrastive_loss_fn = ContrastiveLoss(cfg.CONTRASTIVE_TEMP)
        
        train_loader = AugmentedDataLoader(
            X_train, y_train, 
            batch_size=cfg.WAVE_BATCH,
            augment=cfg.USE_AUGMENTATION
        )
        
        for epoch in range(1, cfg.WAVE_EPOCHS + 1):
            model.train()
            loss_sum = 0.0
            batch_count = 0
            
            for xb, yb in train_loader:
                if cfg.USE_FP16:
                    with autocast(dtype=torch.float16):
                        pred, uncertainty, contrastive = model(xb, return_contrastive=True)
                        
                        main_loss = torch.mean((pred - yb) ** 2 / (uncertainty + 1e-6))
                        
                        if cfg.USE_CONTRASTIVE:
                            y_labels = (torch.sign(yb[:, 0]) + 1).long().clamp(0, 2)
                            cont_loss = contrastive_loss_fn(contrastive, y_labels)
                            loss = main_loss + cfg.CONTRASTIVE_WEIGHT * cont_loss
                        else:
                            loss = main_loss
                    
                    optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    pred, uncertainty, contrastive = model(xb, return_contrastive=True)
                    main_loss = torch.mean((pred - yb) ** 2 / (uncertainty + 1e-6))
                    
                    if cfg.USE_CONTRASTIVE:
                        y_labels = (torch.sign(yb[:, 0]) + 1).long().clamp(0, 2)
                        cont_loss = contrastive_loss_fn(contrastive, y_labels)
                        loss = main_loss + cfg.CONTRASTIVE_WEIGHT * cont_loss
                    else:
                        loss = main_loss
                    
                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                
                loss_sum += loss.item()
                batch_count += 1
            
            scheduler.step()
            
            # Validation
            model.eval()
            with torch.no_grad():
                X_val_t = torch.tensor(X_val, dtype=torch.float32, device=device)
                y_val_t = torch.tensor(y_val, dtype=torch.float32, device=device)
                
                val_pred, val_unc, _ = model(X_val_t, return_contrastive=True)
                val_loss = torch.mean((val_pred - y_val_t) ** 2 / (val_unc + 1e-6)).item()
            
            if epoch % 5 == 0:
                avg_train_loss = loss_sum / max(1, batch_count)
                print(f"[WaveTFT-{model_idx}] Ep {epoch}/{cfg.WAVE_EPOCHS} "
                      f"Train={avg_train_loss:.6e} Val={val_loss:.6e}")
        
        synchronize_cuda()
        models.append(model)
    
    print("\n‚úÖ Wave TFT Training Complete!")
    return models


# ================== „É°„Ç§„É≥ ==================

def main_v5():
    """v5„É°„Ç§„É≥ÂÆüË°å"""
    print("\n[Data Loading]")
    
    for csv in cfg.PAIR_CSV_LIST:
        try:
            df = pd.read_csv(csv)
            prices = df["close"].values.astype(np.float32)
        except:
            print(f"‚ö†Ô∏è  {csv} not found, generating synthetic data")
            prices = np.random.randn(cfg.MAX_POINTS).cumsum() * 0.001 + 150
        
        prices = np.maximum(prices, 1)
        
        print(f"\n[Training] {csv}")
        models = train_forecasters_v5(prices)
        print(f"‚úÖ Trained {len(models)} models on {len(prices)} price points")
    
    print("\n" + "="*80)
    print("üöÄ v5 UBER Edition Ready!")
    print("="*80)


if __name__ == "__main__":
    main_v5()
